{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Olivier Philippon's devblog \u00b6 After more than 20 years spent in web development without having ever taken notes of all the \"tech-wise\" stuff I have in my head, its time to ease my own cognitive overload and start taking notes These notes are mostly for myself, for future reference - but who knows, they might as well be useful to other people Blog posts: 2022-07-28 -- Making SQLite much faster in a local dev environment 2022-07-26 -- Triggering a GitHub Action from an external source 2022-07-22 -- 'From scratch to online in production' in a single day, with Django - Part 2 2022-07-19 -- 'From scratch to online in production' in a single day, with Django - Part 1 2022-07-12 -- Building a Markdown-based blog 2022-07-10 -- Choosing a tech stack for my card game platform","title":"Homepage"},{"location":"#olivier-philippons-devblog","text":"After more than 20 years spent in web development without having ever taken notes of all the \"tech-wise\" stuff I have in my head, its time to ease my own cognitive overload and start taking notes These notes are mostly for myself, for future reference - but who knows, they might as well be useful to other people Blog posts: 2022-07-28 -- Making SQLite much faster in a local dev environment 2022-07-26 -- Triggering a GitHub Action from an external source 2022-07-22 -- 'From scratch to online in production' in a single day, with Django - Part 2 2022-07-19 -- 'From scratch to online in production' in a single day, with Django - Part 1 2022-07-12 -- Building a Markdown-based blog 2022-07-10 -- Choosing a tech stack for my card game platform","title":"Olivier Philippon's devblog"},{"location":"tags/","text":"Tags \u00b6 Following is a list of relevant tags: TIL \u00b6 Triggering a GitHub Action from an external source Making SQLite much faster in a local dev environment blog \u00b6 Building a Markdown-based blog django \u00b6 Choosing a tech stack for my card game platform 'From scratch to online in production' in a single day, with Django - Part 1 'From scratch to online in production' in a single day, with Django - Part 2 Making SQLite much faster in a local dev environment github \u00b6 Triggering a GitHub Action from an external source golang \u00b6 Choosing a tech stack for my card game platform laravel \u00b6 Choosing a tech stack for my card game platform mkdocs \u00b6 Building a Markdown-based blog next.js \u00b6 Choosing a tech stack for my card game platform project layout \u00b6 'From scratch to online in production' in a single day, with Django - Part 1 'From scratch to online in production' in a single day, with Django - Part 2 python \u00b6 Building a Markdown-based blog rails \u00b6 Choosing a tech stack for my card game platform sqlite \u00b6 Making SQLite much faster in a local dev environment","title":"Tags"},{"location":"tags/#tags","text":"Following is a list of relevant tags:","title":"Tags"},{"location":"tags/#til","text":"Triggering a GitHub Action from an external source Making SQLite much faster in a local dev environment","title":"TIL"},{"location":"tags/#blog","text":"Building a Markdown-based blog","title":"blog"},{"location":"tags/#django","text":"Choosing a tech stack for my card game platform 'From scratch to online in production' in a single day, with Django - Part 1 'From scratch to online in production' in a single day, with Django - Part 2 Making SQLite much faster in a local dev environment","title":"django"},{"location":"tags/#github","text":"Triggering a GitHub Action from an external source","title":"github"},{"location":"tags/#golang","text":"Choosing a tech stack for my card game platform","title":"golang"},{"location":"tags/#laravel","text":"Choosing a tech stack for my card game platform","title":"laravel"},{"location":"tags/#mkdocs","text":"Building a Markdown-based blog","title":"mkdocs"},{"location":"tags/#nextjs","text":"Choosing a tech stack for my card game platform","title":"next.js"},{"location":"tags/#project-layout","text":"'From scratch to online in production' in a single day, with Django - Part 1 'From scratch to online in production' in a single day, with Django - Part 2","title":"project layout"},{"location":"tags/#python","text":"Building a Markdown-based blog","title":"python"},{"location":"tags/#rails","text":"Choosing a tech stack for my card game platform","title":"rails"},{"location":"tags/#sqlite","text":"Making SQLite much faster in a local dev environment","title":"sqlite"},{"location":"2022/07-10---choosing-a-tech-stack-for-my-card-game-platform/","tags":["next.js","django","laravel","rails","golang"],"text":"Choosing a tech stack for my card game platform \u00b6 Here are some of the technologies I have professional experience with, in no particular order. Let's check their pros and cons for this card game platform project I want to start building... Note that these bullets points are strictly subjective and personal - I think that all these stacks are perfectly good and valid, and choosing one or another is always a good option anyhow! Next.js \u00b6 Pros Modern, evolving at a quick pace, tons of best practices built-in. Very well documented. I love TypeScript, I like React Now that it's rather mainstream there is a huge ecosystem for it One language (TypeScript) to rule them all! With new runtime such as Cloudflare Workers, Deno and Bun coming in, and all converging towards the use of standard Web APIs, it's a quite exciting time for JavaScript on the backend! Cons Even though it can be used a fullstack framework (as Theo, from ping.gg fame, explains here ) , and despite the numerous benefits of Node.js... I can't help thinking that Node.js doesn't give me the same level of productivity than what I can have by using a \"fully featured\" mature backend framework such as Rails, Laravel or Django - where all the core features such as a typical backend, like an ORM, database migrations, logging, etc, are features that are already plugged together out of the box I gave a quick shot at Prisma , the trendy Node.js ORM; even though it has some undeniable qualities I didn't really fall in love with it - for various reasons. Laravel \u00b6 Pros Probably the framework that comes with the highest number of features built-in. Asynchronous jobs, websockets, authentication, support of Vite.js, you name it... Laravel has it by default. Easy deployment: whether it's via Forge or Vapor, Laravel comes with charged-but-very-handy solutions for deployment. Cons PHP has been my main programming language for backend stuff for more than 15 years, and even though it's never been better than today I grew a bit tired of its $peculiarities Ruby On Rails \u00b6 Pros Almost as \"all features built-in\" as Laravel Huge and mature ecosystem Ruby is a very expressive language Cons After having worked with languages that have inlined type annotations during the last few years, I struggle with languages like Ruby which don't have this So much magic that it can be really hard sometimes to trace what method comes from where and really understand what's going on Django \u00b6 Pros Python My favourite ORM, with (for me) the right balance between power and pragmatism. Also comes with great support for modern database features, such as JSON operators. It's been my tech stack for the last 4 years, so I'm pretty productive with it Has excellent GraphQL libraries (and I love GraphQL! ^_^) The Django Admin website is a huge gain of time while prototyping stuff Cons Some old-school aspects (no routing via HTTP verbs for example - which can be solved by using Django REST Framework, but for some reason I've never really liked it ) Features such as websockets and asynchronous jobs are not built-in, but Django's ecosystem is vast enough to be able to add them with a good integrations ( Channels for the former and Dramatiq for the latter) Go \u00b6 Pros I love the minimalism of the language Very stable over time Strongly typed, with a very smart compiler It's really nice to work with a programming language that have features such as code formatting or testing built-in Cons Whether it's at the database or the GraphQL layer, null values have to be handled and I'm not a big fans of the solutions Go has to offer for that ( sql.NullString and friends, or pointers to primitive values) Can be really verbose sometimes","title":"Choosing a tech stack for my card game platform"},{"location":"2022/07-10---choosing-a-tech-stack-for-my-card-game-platform/#choosing-a-tech-stack-for-my-card-game-platform","text":"Here are some of the technologies I have professional experience with, in no particular order. Let's check their pros and cons for this card game platform project I want to start building... Note that these bullets points are strictly subjective and personal - I think that all these stacks are perfectly good and valid, and choosing one or another is always a good option anyhow!","title":"Choosing a tech stack for my card game platform"},{"location":"2022/07-10---choosing-a-tech-stack-for-my-card-game-platform/#nextjs","text":"Pros Modern, evolving at a quick pace, tons of best practices built-in. Very well documented. I love TypeScript, I like React Now that it's rather mainstream there is a huge ecosystem for it One language (TypeScript) to rule them all! With new runtime such as Cloudflare Workers, Deno and Bun coming in, and all converging towards the use of standard Web APIs, it's a quite exciting time for JavaScript on the backend! Cons Even though it can be used a fullstack framework (as Theo, from ping.gg fame, explains here ) , and despite the numerous benefits of Node.js... I can't help thinking that Node.js doesn't give me the same level of productivity than what I can have by using a \"fully featured\" mature backend framework such as Rails, Laravel or Django - where all the core features such as a typical backend, like an ORM, database migrations, logging, etc, are features that are already plugged together out of the box I gave a quick shot at Prisma , the trendy Node.js ORM; even though it has some undeniable qualities I didn't really fall in love with it - for various reasons.","title":"Next.js"},{"location":"2022/07-10---choosing-a-tech-stack-for-my-card-game-platform/#laravel","text":"Pros Probably the framework that comes with the highest number of features built-in. Asynchronous jobs, websockets, authentication, support of Vite.js, you name it... Laravel has it by default. Easy deployment: whether it's via Forge or Vapor, Laravel comes with charged-but-very-handy solutions for deployment. Cons PHP has been my main programming language for backend stuff for more than 15 years, and even though it's never been better than today I grew a bit tired of its $peculiarities","title":"Laravel"},{"location":"2022/07-10---choosing-a-tech-stack-for-my-card-game-platform/#ruby-on-rails","text":"Pros Almost as \"all features built-in\" as Laravel Huge and mature ecosystem Ruby is a very expressive language Cons After having worked with languages that have inlined type annotations during the last few years, I struggle with languages like Ruby which don't have this So much magic that it can be really hard sometimes to trace what method comes from where and really understand what's going on","title":"Ruby On Rails"},{"location":"2022/07-10---choosing-a-tech-stack-for-my-card-game-platform/#django","text":"Pros Python My favourite ORM, with (for me) the right balance between power and pragmatism. Also comes with great support for modern database features, such as JSON operators. It's been my tech stack for the last 4 years, so I'm pretty productive with it Has excellent GraphQL libraries (and I love GraphQL! ^_^) The Django Admin website is a huge gain of time while prototyping stuff Cons Some old-school aspects (no routing via HTTP verbs for example - which can be solved by using Django REST Framework, but for some reason I've never really liked it ) Features such as websockets and asynchronous jobs are not built-in, but Django's ecosystem is vast enough to be able to add them with a good integrations ( Channels for the former and Dramatiq for the latter)","title":"Django"},{"location":"2022/07-10---choosing-a-tech-stack-for-my-card-game-platform/#go","text":"Pros I love the minimalism of the language Very stable over time Strongly typed, with a very smart compiler It's really nice to work with a programming language that have features such as code formatting or testing built-in Cons Whether it's at the database or the GraphQL layer, null values have to be handled and I'm not a big fans of the solutions Go has to offer for that ( sql.NullString and friends, or pointers to primitive values) Can be really verbose sometimes","title":"Go"},{"location":"2022/07-12---building-a-markdown-based-blog/","tags":["blog","mkdocs","python"],"text":"I've put this devblog online the other day, but haven't documented how I built it yet - which will be useful for my future self if I have to do something like this again, and could potentially be of some help for other people. Here is how I determined what was the quickest way (for me) to put a \"devblog\" online as quickly as possible, without hours of setup. Why not Next.js? \u00b6 My natural \"go-to\" option would be Next.js, with its SSG capacities and built-in support of MDX . Also, I like managing this kind of stuff manually myself - i.e. traversing a folder of Markdown files at deployment time when Vercel runs npm run build , sorting my content, providing the result to getStaticProps ... It's genuinely fun However, even though I really enjoy doing this kind of work on the Node.js side there is a drawback for me: I would then have to build the user interface myself. And my skills in terms of design being as bad as they can be , I really don't want to work for hours on the React side only to end up with an ugly UI. There are open source or free turnkey blog templates for React/Next.js, of course, but I haven't really seen any of their design that I like (that's obviously highly subjective). Hence my second option... Material for MKDocs \u00b6 Or: Using a documentation engine to make a blog, what could go wrong? (spoiler: it was quick and fun to do, I regret nothing :-) MkDocs and its modern Material theme \u00b6 MkDocs is a static documentation generator, programmed in Python and configurable in YAML, created (if I'm not wrong) by Tom Christie - who's also behind huge Python projects like Django REST Framework or Starlette . Question I don't know the history of MkDocs, but I guess Tom Christie created it to document Django REST Framework? Material for MKDocs is a theme for MKDocs, based on Google's Material UI guidelines, that make MKDocs look much more modern (as great as the project is, MKDocs' default theme shows its age nowadays). Why they can be relevant to build a simple devblog \u00b6 Neither MKDocs or Material for MKDocs are designed to make blogs (even though it's on the roadmap ), but they have some common aspects with what I wanted to do: Treats folders of Markdown files (with YAML metadata in their header) as a tree of Web pages Built-in ability to publish static pages to GitHub Pages , for free Simple but good-looking user interface theme by default , so I don't have to fiddle with HTML and CSS but can focus on the content itself Made with a technology I'm familiar with (Python in this case) Excellent syntax highlighting (will be useful for my snippets :-), powered by Pygments The plugin mkdocs-awesome-pages-plugin also helps, so I don't have to build the whole navigation tree manually in the mkdocs.yml file Quick setup of \" Material for MKDocs as a blog\" \u00b6 I found two good resources explaining how to use Material for MKDocs as a blog engine, so I could hit the ground running (I really didn't want to spend hours on this setup ): https://www.dirigible.io/blogs/2021/11/2/material-blogging-capabilities/ https://ultrabug.fr/Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/ The documentation of Material for MKDocs itself if really nice, and pragmatic - explaining very simply for example how one can publish the generated static HTML content to GitHub Pages: https://squidfunk.github.io/mkdocs-material/publishing-your-site It was looking doable! :-) Let's give it a shot, with a quick Python setup: $ mkdir devblog && cd devblog/ $ pyenv shell 3 .10.4 # (1) $ python -m venv .venv # (2) $ source .venv/bin/activate # (3) ( .venv ) $ pip install -U pip poetry # (4) ( .venv ) $ poetry init # (5) ( .venv ) $ poetry add \\ # (6) mkdocs-material \\ mkdocs-awesome-pages-plugin let's use a recent version of Python create a virtual env in a \".venv\" folder activate the virtual env: from now on the Python-related commands we type only impact the \".venv\" folder in the virtual env, update pip and install the Poetry package manager initialise Poetry for this project ask Poetry to install the few packages we need for this blog Now all I had to do was to follow what these 2 articles were explaining, browse a bit the Material for MKDocs documentation, use the really nice icons and emojis search provided by this documentation (it's the little things ^_^)... and a couple of hours later my first blog post was online, automatically published by a GitHub Action every time I push my main branch! My quick personal touch \u00b6 Compared to these two very useful articles, my only personal touch was to add two things I always use in my projects: The modern package manager Poetry rather than pip (the Python default one) Create a Makefile at the root of the git repository, for the common tasks I also wanted to automate the \"posts table of content\" on the blog homepage. I chose to do it myself, mainly because it's the kind of things I really enjoy coding ^_^ The logic lives in the my_plugins/blog_toc/hooks.py Python file, plugged to the MKDocs generation lifecycle with the nice mkdocs-simple-hooks package for the sake of simplicity Later on I added the generation of a RSS feed when the blog is built, powered by the mkdocs-rss-plugin package.","title":"Building a Markdown-based blog"},{"location":"2022/07-12---building-a-markdown-based-blog/#why-not-nextjs","text":"My natural \"go-to\" option would be Next.js, with its SSG capacities and built-in support of MDX . Also, I like managing this kind of stuff manually myself - i.e. traversing a folder of Markdown files at deployment time when Vercel runs npm run build , sorting my content, providing the result to getStaticProps ... It's genuinely fun However, even though I really enjoy doing this kind of work on the Node.js side there is a drawback for me: I would then have to build the user interface myself. And my skills in terms of design being as bad as they can be , I really don't want to work for hours on the React side only to end up with an ugly UI. There are open source or free turnkey blog templates for React/Next.js, of course, but I haven't really seen any of their design that I like (that's obviously highly subjective). Hence my second option...","title":"Why not Next.js?"},{"location":"2022/07-12---building-a-markdown-based-blog/#material-for-mkdocs","text":"Or: Using a documentation engine to make a blog, what could go wrong? (spoiler: it was quick and fun to do, I regret nothing :-)","title":"Material for MKDocs"},{"location":"2022/07-12---building-a-markdown-based-blog/#mkdocs-and-its-modern-material-theme","text":"MkDocs is a static documentation generator, programmed in Python and configurable in YAML, created (if I'm not wrong) by Tom Christie - who's also behind huge Python projects like Django REST Framework or Starlette . Question I don't know the history of MkDocs, but I guess Tom Christie created it to document Django REST Framework? Material for MKDocs is a theme for MKDocs, based on Google's Material UI guidelines, that make MKDocs look much more modern (as great as the project is, MKDocs' default theme shows its age nowadays).","title":"MkDocs and its modern Material theme"},{"location":"2022/07-12---building-a-markdown-based-blog/#why-they-can-be-relevant-to-build-a-simple-devblog","text":"Neither MKDocs or Material for MKDocs are designed to make blogs (even though it's on the roadmap ), but they have some common aspects with what I wanted to do: Treats folders of Markdown files (with YAML metadata in their header) as a tree of Web pages Built-in ability to publish static pages to GitHub Pages , for free Simple but good-looking user interface theme by default , so I don't have to fiddle with HTML and CSS but can focus on the content itself Made with a technology I'm familiar with (Python in this case) Excellent syntax highlighting (will be useful for my snippets :-), powered by Pygments The plugin mkdocs-awesome-pages-plugin also helps, so I don't have to build the whole navigation tree manually in the mkdocs.yml file","title":"Why they can be relevant to build a simple devblog"},{"location":"2022/07-12---building-a-markdown-based-blog/#quick-setup-of-material-for-mkdocs-as-a-blog","text":"I found two good resources explaining how to use Material for MKDocs as a blog engine, so I could hit the ground running (I really didn't want to spend hours on this setup ): https://www.dirigible.io/blogs/2021/11/2/material-blogging-capabilities/ https://ultrabug.fr/Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/ The documentation of Material for MKDocs itself if really nice, and pragmatic - explaining very simply for example how one can publish the generated static HTML content to GitHub Pages: https://squidfunk.github.io/mkdocs-material/publishing-your-site It was looking doable! :-) Let's give it a shot, with a quick Python setup: $ mkdir devblog && cd devblog/ $ pyenv shell 3 .10.4 # (1) $ python -m venv .venv # (2) $ source .venv/bin/activate # (3) ( .venv ) $ pip install -U pip poetry # (4) ( .venv ) $ poetry init # (5) ( .venv ) $ poetry add \\ # (6) mkdocs-material \\ mkdocs-awesome-pages-plugin let's use a recent version of Python create a virtual env in a \".venv\" folder activate the virtual env: from now on the Python-related commands we type only impact the \".venv\" folder in the virtual env, update pip and install the Poetry package manager initialise Poetry for this project ask Poetry to install the few packages we need for this blog Now all I had to do was to follow what these 2 articles were explaining, browse a bit the Material for MKDocs documentation, use the really nice icons and emojis search provided by this documentation (it's the little things ^_^)... and a couple of hours later my first blog post was online, automatically published by a GitHub Action every time I push my main branch!","title":"Quick setup of \"Material for MKDocs as a blog\""},{"location":"2022/07-12---building-a-markdown-based-blog/#my-quick-personal-touch","text":"Compared to these two very useful articles, my only personal touch was to add two things I always use in my projects: The modern package manager Poetry rather than pip (the Python default one) Create a Makefile at the root of the git repository, for the common tasks I also wanted to automate the \"posts table of content\" on the blog homepage. I chose to do it myself, mainly because it's the kind of things I really enjoy coding ^_^ The logic lives in the my_plugins/blog_toc/hooks.py Python file, plugged to the MKDocs generation lifecycle with the nice mkdocs-simple-hooks package for the sake of simplicity Later on I added the generation of a RSS feed when the blog is built, powered by the mkdocs-rss-plugin package.","title":"My quick personal touch"},{"location":"2022/07-19---from-scratch-to-online-in-production-in-a-single-day-with-django-part-1/","tags":["django","project layout"],"text":"'From scratch to online in production' in a single day, with Django - Part 1 \u00b6 A Gin Rummy leaderboard \u00b6 There is this card game I play a lot with my partner, when we go to the pub or just chill outside: Gin Rummy . If you're curious about the game itself... Gin Rummy is a two-player card game, created by a professional card game player and his son in 1909 - preferably to be played in pubs, as its name made of 2 alcohols' ones hints It's easy to learn, the rounds are pretty quick, and even though randomness always plays an important role in card games its role is not too strong in Gin Rummy. A proof of this is that during the 1970s and 1980s there used to be a champion, Stu Ungar , who had such a total dominance of the game that it actually ended up killing the game in tournaments, since he was pretty much winning all of them! We like playing casually, but we also like trying to do our best while playing, so we had to find a handy way to keep track of our scores to keep a bit of competitiveness. As a Web developer, of course I had to build a leaderboard for us! My challenge: start working on it in the morning, and have it live in production in the afternoon \u00b6 My challenge was to build it in one single day , from scratch in the morning to having it online up and running with a database in the afternoon. The stack I'm the most productive with being Django , I opted for this framework despite the minimalism of the project. Is Django a good choice for small projects? Yes! Contrary to frameworks such a Ruby On Rails or Laravel, which tends to create a lot fo files for a blank project, creating a new Django project with django-admin startproject creates only 6 files , making it a good choice even for small projects. One can even create a Django-powered REST API contained in a single Python file! https://adamj.eu/tech/2020/10/15/a-single-file-rest-api-in-django/ This is the first post explaining how I typically organise the file tree of a Django project - this layout does the job for a micro project such as this Gin Rummy leaderboard, but scales very well for \"real life\" projects too! I have several Django apps running in production for years with this layout, and the pattern scales smoothly as features keep being added to the projects Let's start! My typical bootstrap of a Django app \u00b6 $ mkdir gin-scoring && cd gin-scoring/ $ pyenv shell 3 .10.4 # (1) $ python -m venv .venv # (2) $ source .venv/bin/activate # (3) ( .venv ) $ pip install -U pip poetry # (4) ( .venv ) $ poetry init # (5) ( .venv ) $ poetry add \\ # (6) Django \\ django-environ \\ # (7) psycopg2 \\ # (8) Jinja2 # (9) Let's use a recent version of Python Create a virtual env in a \".venv\" folder Activate the virtual env: from now on the Python-related commands we type only impact the \".venv\" folder In the virtual env, update pip and install the Poetry package manager Initialise Poetry for this project Ask Poetry to install the few packages we need for this blog: Install django-environ to manage our settings I'll be using Postgresql for the database, so we need a Python driver for it I was never a big fan of the Django built-in templating language, so I always use Jinja instead Installing pip and Poetry in the virtual environment These 2 tools can be used globally , and do not require such a local installation. The reason I do this is just because I like having all my Python projects entirely self-contained in their respective virtual envs. So if one of my project uses Poetry 2.x one day, for example, I'll be able to use it on new projects without messing up my existing ones Once we have this we can bootstrap the project: ( .venv ) $ mkdir src && cd src/ # (1) ( .venv ) $ django-admin startproject \"project\" . I always put my Python files in a \"src/\" folder With this startproject command we create the skeleton of a Django project in the current folder ( src/ ), with a project named \"project\" - I always choose this name because of the way I handle my Django settings. I like having a apps/ and a project/ folders in my src/ one: the former will be the Python package where all my Django app code lives, while the latter is where I'll store the project-wide settings. In a nutshell, the file tree I want to have is this: gin-scoring/ \u251c\u2500\u2500 src/ \u2502 \u251c\u2500\u2500 apps/ \u2502 \u2502 \u251c\u2500\u2500 authentication/ # (1) \u2502 \u2502 \u2502 \u251c\u2500\u2500 migrations/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 admin.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 apps.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 models.py \u2502 \u2502 \u2514\u2500\u2500 gin_scoring/ # (2) \u2502 \u2502 \u2502 \u251c\u2500\u2500 domain/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 jinja2/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 migrations/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 admin.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 apps.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 helpers.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 http_payloads.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 models.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 urls.py \u2502 \u251c\u2500\u2500 project/ # (3) \u2502 \u2502 \u251c\u2500\u2500 settings/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 _base.py # (4) \u2502 \u2502 \u2502 \u251c\u2500\u2500 development.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 flyio.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 heroku.py \u2502 \u2502 \u251c\u2500\u2500 asgi.py # (5) \u2502 \u2502 \u251c\u2500\u2500 jinja2.py \u2502 \u2502 \u251c\u2500\u2500 urls.py \u2502 \u2502 \u2514\u2500\u2500 wsgi.py # (6) \u2502 \u2514\u2500\u2500 manage.py* # (7) \u251c\u2500\u2500 tests/ \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 poetry.lock \u2514\u2500\u2500 pyproject.toml This will be a apps.authentication package This will be a apps.gin_scoring package - where the business logic of this mini project will be :-) This is why I use \"project\" for the name of my Django project when I boostrap it with django-admin startproject The \"settings\" file generated by django-admin startproject : I just move it in this folder and rename it into _base.py The ASGI (async Python) entry point of the project, generated by Django - I'm not using it at the moment, but it could be useful later on so let's keep it :-) The WSGI (\"traditional\" Python) entry point: that's where all the HTTP requests of this app will be processed The classic command line entry point for Django - from now on I will always use python src/manage.py for my Django commands, and startproject was the only one for which I used django-admin (file tree generated with tree --dirsfirst -I __pycache__ -F -L 4 . - see tree 's MAN page) Having all my Django apps namespaced in this apps package allows me to use pretty much any name for them, without any risks of collisions with a 3rd-party package. Why I namespace my Python code In environments such as PHP or Node.js the 3rd-party packages we add to a project are always namespaced, so there are no risks of collisions with our own code. On a Laravel project for example, the HTTP request class will have the fully-qualified name Illuminate\\Http\\Request ; so we're free to have our own Request class with pretty much any prefix we want, if we need one in order to follow the \"domain\" glossary of the project we're building. However, in the Python world there is not only no namespacing, but also no constrained matching between the name of a 3rd-party package we add to a project and its Python package . For example, the package django-environ lives in a package named environ . This is why I tend to be a bit defensive when it comes to namespace my own code In that aspect I actually imitate what a typical Laravel project does, since everything is namespaced in a top-level \\App\\ namespace there: https://laravel.com/docs/9.x/structure#the-app-directory I can still have collisions in the Django apps names though - but it's a less annoying risk. Example of a Django apps names collision Django comes with a handy auth app that I want to use, which I why I cannot create an app that have this name myself - hence my longer apps.authentication naming Django settings management \u00b6 The core of my Django settings are in the file src/project/settings/_base.py : this is just the settings file generated by django-admin startproject , that I moved into a new settings/ folder and renamed - the leading underscore is just common a convention to emphasise that this Python module shouldn't itself be imported. It all starts with a _base \u00b6 The content of this file looks like this: import os from pathlib import Path import environ # This points to our git repo's root: BASE_DIR = Path ( __file__ ) . parent . resolve () / \"..\" / \"..\" / \"..\" env = environ . Env () if os . environ . get ( \"USE_DOT_ENV\" ): # (1) for env_file_name in ( \".env\" , \".env.local\" ): env_file_path = BASE_DIR / env_file_name try : environ . Env . read_env ( env_file_path ) except ( OSError , AttributeError ): pass # no .env file? No problem! SECRET_KEY = env . str ( \"SECRET_KEY\" ) # Classic Django settings, generated by `django-admin startproject` # I'll omit them for brevity INSTALLED_APPS = [ ... ] MIDDLEWARE = [ ... ] ROOT_URLCONF = \"project.urls\" # etc. # Database # https://docs.djangoproject.com/en/4.0/ref/settings/#databases DATABASES = { \"default\" : env . db_url ( \"DATABASE_URL\" ), # (2) } # etc., again... I will only use this in \"local\" development: in production the settings are only set via environment variables And here where django-environ kicks in again: we use a single DATABASE_URL environment var, rather than one setting for the username, one for the password, etc. From there, we just have to define \"environment-specific\" settings. \"Local dev\" settings \u00b6 My local development settings look like this for example: import os # This enables the loading of \".env\" files in local development: os . environ [ \"USE_DOT_ENV\" ] = \"YES\" # N.B. This is the only part of my Python code # where I allow myself \"star imports\" :-) from ._base import * DEBUG = True ALLOWED_HOSTS = [] LOGGING = { \"version\" : 1 , \"disable_existing_loggers\" : False , \"handlers\" : { \"console\" : { \"class\" : \"logging.StreamHandler\" , }, }, \"root\" : { \"handlers\" : [ \"console\" ], \"level\" : env . str ( \"DJANGO_LOG_LEVEL\" , default = \"WARNING\" ), }, \"loggers\" : { \"apps\" : { \"handlers\" : [ \"console\" ], \"level\" : env . str ( \"APP_LOG_LEVEL\" , default = \"INFO\" ), \"propagate\" : False , }, \"django.db.backends\" : { \"handlers\" : [ \"console\" ], \"level\" : env . str ( \"SQL_LOG_LEVEL\" , default = \"WARNING\" ), # (1) \"propagate\" : False , }, }, } Such granular logging is very handy in development mode :-) So if I want to check the SQL queries generated by the Django ORM while I'm working on the project, all I have to do is to launch my server with: ( .venv ) $ SQL_LOG_LEVEL = DEBUG djm runserver The djm shell alias djm is short for \"DJango Management\" - it's an alias I have in my shell's startup file. As I always use this same layout for all my Django projects I can always use the same alias to run my Django commands. alias djm = 'DJANGO_SETTINGS_MODULE=project.settings.development python src/manage.py' So from there I can start my Django server with djm runserver , generate database migrations with djm makemigrations , apply them with djm migrate , etc. The venv shell alias Oh, and while we're there, here is another handy alias: alias venv = 'source .venv/bin/activate' So when I cd into a Python project folder I just have to type venv to activate its virtual environment, as I always create it in a .venv/ folder Production settings \u00b6 My production (Heroku in this case) settings, in the same folder, look like that: from ._base import * ALLOWED_HOSTS = env . list ( \"ALLOWED_HOSTS\" ) DEBUG = False SECURE_SSL_REDIRECT = True CSRF_COOKIE_SECURE = True SESSION_COOKIE_SECURE = True # Static assets served by Whitenoise on production # @link https://devcenter.heroku.com/articles/django-assets # @link http://whitenoise.evans.io/en/stable/ STATIC_ROOT = BASE_DIR / \"staticfiles\" MIDDLEWARE . append ( \"whitenoise.middleware.WhiteNoiseMiddleware\" ) STATICFILES_STORAGE = \"whitenoise.storage.CompressedManifestStaticFilesStorage\" # Logging LOGGING = { \"version\" : 1 , \"disable_existing_loggers\" : False , \"handlers\" : { \"console\" : { \"class\" : \"logging.StreamHandler\" , }, }, \"root\" : { \"handlers\" : [ \"console\" ], \"level\" : \"WARNING\" , }, } In the next post we'll start coding the app itself, using this pattern I was mentioning - which shines by its simplicity and ability to scale as a project gets more and more complex. Thanks to my friend Yann - einenlum.com - for his review on this post Part 2 \u00b6 UPDATE: Part 2 is online","title":"'From scratch to online in production' in a single day, with Django - Part 1"},{"location":"2022/07-19---from-scratch-to-online-in-production-in-a-single-day-with-django-part-1/#from-scratch-to-online-in-production-in-a-single-day-with-django-part-1","text":"","title":"'From scratch to online in production' in a single day, with Django - Part 1"},{"location":"2022/07-19---from-scratch-to-online-in-production-in-a-single-day-with-django-part-1/#a-gin-rummy-leaderboard","text":"There is this card game I play a lot with my partner, when we go to the pub or just chill outside: Gin Rummy . If you're curious about the game itself... Gin Rummy is a two-player card game, created by a professional card game player and his son in 1909 - preferably to be played in pubs, as its name made of 2 alcohols' ones hints It's easy to learn, the rounds are pretty quick, and even though randomness always plays an important role in card games its role is not too strong in Gin Rummy. A proof of this is that during the 1970s and 1980s there used to be a champion, Stu Ungar , who had such a total dominance of the game that it actually ended up killing the game in tournaments, since he was pretty much winning all of them! We like playing casually, but we also like trying to do our best while playing, so we had to find a handy way to keep track of our scores to keep a bit of competitiveness. As a Web developer, of course I had to build a leaderboard for us!","title":"A Gin Rummy leaderboard"},{"location":"2022/07-19---from-scratch-to-online-in-production-in-a-single-day-with-django-part-1/#my-challenge-start-working-on-it-in-the-morning-and-have-it-live-in-production-in-the-afternoon","text":"My challenge was to build it in one single day , from scratch in the morning to having it online up and running with a database in the afternoon. The stack I'm the most productive with being Django , I opted for this framework despite the minimalism of the project. Is Django a good choice for small projects? Yes! Contrary to frameworks such a Ruby On Rails or Laravel, which tends to create a lot fo files for a blank project, creating a new Django project with django-admin startproject creates only 6 files , making it a good choice even for small projects. One can even create a Django-powered REST API contained in a single Python file! https://adamj.eu/tech/2020/10/15/a-single-file-rest-api-in-django/ This is the first post explaining how I typically organise the file tree of a Django project - this layout does the job for a micro project such as this Gin Rummy leaderboard, but scales very well for \"real life\" projects too! I have several Django apps running in production for years with this layout, and the pattern scales smoothly as features keep being added to the projects Let's start!","title":"My challenge: start working on it in the morning, and have it live in production in the afternoon"},{"location":"2022/07-19---from-scratch-to-online-in-production-in-a-single-day-with-django-part-1/#my-typical-bootstrap-of-a-django-app","text":"$ mkdir gin-scoring && cd gin-scoring/ $ pyenv shell 3 .10.4 # (1) $ python -m venv .venv # (2) $ source .venv/bin/activate # (3) ( .venv ) $ pip install -U pip poetry # (4) ( .venv ) $ poetry init # (5) ( .venv ) $ poetry add \\ # (6) Django \\ django-environ \\ # (7) psycopg2 \\ # (8) Jinja2 # (9) Let's use a recent version of Python Create a virtual env in a \".venv\" folder Activate the virtual env: from now on the Python-related commands we type only impact the \".venv\" folder In the virtual env, update pip and install the Poetry package manager Initialise Poetry for this project Ask Poetry to install the few packages we need for this blog: Install django-environ to manage our settings I'll be using Postgresql for the database, so we need a Python driver for it I was never a big fan of the Django built-in templating language, so I always use Jinja instead Installing pip and Poetry in the virtual environment These 2 tools can be used globally , and do not require such a local installation. The reason I do this is just because I like having all my Python projects entirely self-contained in their respective virtual envs. So if one of my project uses Poetry 2.x one day, for example, I'll be able to use it on new projects without messing up my existing ones Once we have this we can bootstrap the project: ( .venv ) $ mkdir src && cd src/ # (1) ( .venv ) $ django-admin startproject \"project\" . I always put my Python files in a \"src/\" folder With this startproject command we create the skeleton of a Django project in the current folder ( src/ ), with a project named \"project\" - I always choose this name because of the way I handle my Django settings. I like having a apps/ and a project/ folders in my src/ one: the former will be the Python package where all my Django app code lives, while the latter is where I'll store the project-wide settings. In a nutshell, the file tree I want to have is this: gin-scoring/ \u251c\u2500\u2500 src/ \u2502 \u251c\u2500\u2500 apps/ \u2502 \u2502 \u251c\u2500\u2500 authentication/ # (1) \u2502 \u2502 \u2502 \u251c\u2500\u2500 migrations/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 admin.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 apps.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 models.py \u2502 \u2502 \u2514\u2500\u2500 gin_scoring/ # (2) \u2502 \u2502 \u2502 \u251c\u2500\u2500 domain/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 jinja2/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 migrations/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 admin.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 apps.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 helpers.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 http_payloads.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 models.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 urls.py \u2502 \u251c\u2500\u2500 project/ # (3) \u2502 \u2502 \u251c\u2500\u2500 settings/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 _base.py # (4) \u2502 \u2502 \u2502 \u251c\u2500\u2500 development.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 flyio.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 heroku.py \u2502 \u2502 \u251c\u2500\u2500 asgi.py # (5) \u2502 \u2502 \u251c\u2500\u2500 jinja2.py \u2502 \u2502 \u251c\u2500\u2500 urls.py \u2502 \u2502 \u2514\u2500\u2500 wsgi.py # (6) \u2502 \u2514\u2500\u2500 manage.py* # (7) \u251c\u2500\u2500 tests/ \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 poetry.lock \u2514\u2500\u2500 pyproject.toml This will be a apps.authentication package This will be a apps.gin_scoring package - where the business logic of this mini project will be :-) This is why I use \"project\" for the name of my Django project when I boostrap it with django-admin startproject The \"settings\" file generated by django-admin startproject : I just move it in this folder and rename it into _base.py The ASGI (async Python) entry point of the project, generated by Django - I'm not using it at the moment, but it could be useful later on so let's keep it :-) The WSGI (\"traditional\" Python) entry point: that's where all the HTTP requests of this app will be processed The classic command line entry point for Django - from now on I will always use python src/manage.py for my Django commands, and startproject was the only one for which I used django-admin (file tree generated with tree --dirsfirst -I __pycache__ -F -L 4 . - see tree 's MAN page) Having all my Django apps namespaced in this apps package allows me to use pretty much any name for them, without any risks of collisions with a 3rd-party package. Why I namespace my Python code In environments such as PHP or Node.js the 3rd-party packages we add to a project are always namespaced, so there are no risks of collisions with our own code. On a Laravel project for example, the HTTP request class will have the fully-qualified name Illuminate\\Http\\Request ; so we're free to have our own Request class with pretty much any prefix we want, if we need one in order to follow the \"domain\" glossary of the project we're building. However, in the Python world there is not only no namespacing, but also no constrained matching between the name of a 3rd-party package we add to a project and its Python package . For example, the package django-environ lives in a package named environ . This is why I tend to be a bit defensive when it comes to namespace my own code In that aspect I actually imitate what a typical Laravel project does, since everything is namespaced in a top-level \\App\\ namespace there: https://laravel.com/docs/9.x/structure#the-app-directory I can still have collisions in the Django apps names though - but it's a less annoying risk. Example of a Django apps names collision Django comes with a handy auth app that I want to use, which I why I cannot create an app that have this name myself - hence my longer apps.authentication naming","title":"My typical bootstrap of a Django app"},{"location":"2022/07-19---from-scratch-to-online-in-production-in-a-single-day-with-django-part-1/#django-settings-management","text":"The core of my Django settings are in the file src/project/settings/_base.py : this is just the settings file generated by django-admin startproject , that I moved into a new settings/ folder and renamed - the leading underscore is just common a convention to emphasise that this Python module shouldn't itself be imported.","title":"Django settings management"},{"location":"2022/07-19---from-scratch-to-online-in-production-in-a-single-day-with-django-part-1/#it-all-starts-with-a-_base","text":"The content of this file looks like this: import os from pathlib import Path import environ # This points to our git repo's root: BASE_DIR = Path ( __file__ ) . parent . resolve () / \"..\" / \"..\" / \"..\" env = environ . Env () if os . environ . get ( \"USE_DOT_ENV\" ): # (1) for env_file_name in ( \".env\" , \".env.local\" ): env_file_path = BASE_DIR / env_file_name try : environ . Env . read_env ( env_file_path ) except ( OSError , AttributeError ): pass # no .env file? No problem! SECRET_KEY = env . str ( \"SECRET_KEY\" ) # Classic Django settings, generated by `django-admin startproject` # I'll omit them for brevity INSTALLED_APPS = [ ... ] MIDDLEWARE = [ ... ] ROOT_URLCONF = \"project.urls\" # etc. # Database # https://docs.djangoproject.com/en/4.0/ref/settings/#databases DATABASES = { \"default\" : env . db_url ( \"DATABASE_URL\" ), # (2) } # etc., again... I will only use this in \"local\" development: in production the settings are only set via environment variables And here where django-environ kicks in again: we use a single DATABASE_URL environment var, rather than one setting for the username, one for the password, etc. From there, we just have to define \"environment-specific\" settings.","title":"It all starts with a _base"},{"location":"2022/07-19---from-scratch-to-online-in-production-in-a-single-day-with-django-part-1/#local-dev-settings","text":"My local development settings look like this for example: import os # This enables the loading of \".env\" files in local development: os . environ [ \"USE_DOT_ENV\" ] = \"YES\" # N.B. This is the only part of my Python code # where I allow myself \"star imports\" :-) from ._base import * DEBUG = True ALLOWED_HOSTS = [] LOGGING = { \"version\" : 1 , \"disable_existing_loggers\" : False , \"handlers\" : { \"console\" : { \"class\" : \"logging.StreamHandler\" , }, }, \"root\" : { \"handlers\" : [ \"console\" ], \"level\" : env . str ( \"DJANGO_LOG_LEVEL\" , default = \"WARNING\" ), }, \"loggers\" : { \"apps\" : { \"handlers\" : [ \"console\" ], \"level\" : env . str ( \"APP_LOG_LEVEL\" , default = \"INFO\" ), \"propagate\" : False , }, \"django.db.backends\" : { \"handlers\" : [ \"console\" ], \"level\" : env . str ( \"SQL_LOG_LEVEL\" , default = \"WARNING\" ), # (1) \"propagate\" : False , }, }, } Such granular logging is very handy in development mode :-) So if I want to check the SQL queries generated by the Django ORM while I'm working on the project, all I have to do is to launch my server with: ( .venv ) $ SQL_LOG_LEVEL = DEBUG djm runserver The djm shell alias djm is short for \"DJango Management\" - it's an alias I have in my shell's startup file. As I always use this same layout for all my Django projects I can always use the same alias to run my Django commands. alias djm = 'DJANGO_SETTINGS_MODULE=project.settings.development python src/manage.py' So from there I can start my Django server with djm runserver , generate database migrations with djm makemigrations , apply them with djm migrate , etc. The venv shell alias Oh, and while we're there, here is another handy alias: alias venv = 'source .venv/bin/activate' So when I cd into a Python project folder I just have to type venv to activate its virtual environment, as I always create it in a .venv/ folder","title":"\"Local dev\" settings"},{"location":"2022/07-19---from-scratch-to-online-in-production-in-a-single-day-with-django-part-1/#production-settings","text":"My production (Heroku in this case) settings, in the same folder, look like that: from ._base import * ALLOWED_HOSTS = env . list ( \"ALLOWED_HOSTS\" ) DEBUG = False SECURE_SSL_REDIRECT = True CSRF_COOKIE_SECURE = True SESSION_COOKIE_SECURE = True # Static assets served by Whitenoise on production # @link https://devcenter.heroku.com/articles/django-assets # @link http://whitenoise.evans.io/en/stable/ STATIC_ROOT = BASE_DIR / \"staticfiles\" MIDDLEWARE . append ( \"whitenoise.middleware.WhiteNoiseMiddleware\" ) STATICFILES_STORAGE = \"whitenoise.storage.CompressedManifestStaticFilesStorage\" # Logging LOGGING = { \"version\" : 1 , \"disable_existing_loggers\" : False , \"handlers\" : { \"console\" : { \"class\" : \"logging.StreamHandler\" , }, }, \"root\" : { \"handlers\" : [ \"console\" ], \"level\" : \"WARNING\" , }, } In the next post we'll start coding the app itself, using this pattern I was mentioning - which shines by its simplicity and ability to scale as a project gets more and more complex. Thanks to my friend Yann - einenlum.com - for his review on this post","title":"Production settings"},{"location":"2022/07-19---from-scratch-to-online-in-production-in-a-single-day-with-django-part-1/#part-2","text":"UPDATE: Part 2 is online","title":"Part 2"},{"location":"2022/07-22---from-scratch-to-online-in-production-in-a-single-day-with-django-part-2/","tags":["django","project layout"],"text":"'From scratch to online in production' in a single day, with Django - Part 2 \u00b6 Quick summary of the previous part \u00b6 In part 1 we saw a way to set up a Django project in a quite generic way, that can be used for any kind of project: Dependencies are managed by Poetry Settings that differ from an environment to another come from environment variables, and are populated from optional .env files in \"local development\" mode, powered by django-environ . SECRET_KEY = env . str ( \"SECRET_KEY\" ) DATABASES = { \"default\" : env . db_url ( \"DATABASE_URL\" ), } On top of this we also have a Python module for each type of environment - so we'll use the DJANGO_SETTINGS_MODULE environment variable to point to project.settings.development during local development , project.settings.heroku on Heroku, project.settings.test when running our test suite, etc. Info Having such multiple settings files is a best practice I learned by reading the great book Two Scoops of Django - which itself cites this talk from Jacob Kaplan-Moss. A src/ folder contains all the application code, within 2 top-level packages for our Python modules: project , which contains the Django settings and the WSGI/ASGI HTTP entrypoints apps for the Django apps - so each of them is free to have any name we want, without any risks of collisions with an existing 3rd-party package. Next step: we have to organise the business logic inside our Django apps \u00b6 Django has some built-in recommendations about how to structure a project. Its applications concept for example is a good start to split the code into smaller units, and some basic rules like this one are good guidelines: Quote If there are 20+ models in a single app, think about ways to break it down into smaller apps, as it probably means your app is doing too much. In practice, we like to lower this number to no more than five to ten models per app . From Two Scoops of Django Now, let's take a look at an aspect that may be a mystery for other developers who enter this new ecosystem they're not familiar with yet - like me a few years ago, when I switched from \"PHP + Symfony \" to \"Python + Django \" : How to organise the business logic inside each Django app? Me, as I was learning Django Well, I guess \"the Django way\" would probably be to follow the Active Record pattern and write most of the code for this in the Models themselves - as well as in their Managers (which are similar to \"repositories\" or \"entity managers\" when using the Data Mapper pattern) However, in my own (subjective) case I generally find that doing so doesn't scale very well as the project grows, as we end up having all the business logic grouped in a bunch of huge classes. Which is why, as I was learning Django, I was quite eager to find another way to structure my code... An efficient pattern for the business logic: simplicity that scales well \u00b6 When I started to learn Django I was lucky enough to stumble upon this Django Styleguide , published on GitHub by the software development company HackSoft. And more specifically this part, where they explain \"services\" and \"selectors\": https://github.com/HackSoftware/Django-Styleguide#services My own adaptation of the \"services\" and \"selectors\" concept from HackSoft \u00b6 As I'm mostly working with GraphQL in my day-to-day job, I opted for a terminology that rings a bell a bit more to my ears than \"services\" and \"selectors\" : mutations and queries . The former is a package that contains code that alter a database (adding, modifying or deleting data from it), while the latter is specialised in fetching data. Here is how it looks like applied to my \"Gin Rummy leaderboard\" mini project: (the parts of the tree that don't matter in this case are replaced with three dots) gin-scoring/ \u251c\u2500\u2500 src/ \u2502 \u251c\u2500\u2500 apps/ \u2502 \u2502 \u251c\u2500\u2500 authentication/ \u2502 \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2502 \u251c\u2500\u2500 gin_scoring/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 domain/ \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 mutations/ # (1) \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 _save_game_result.py \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 queries/ # (2) \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 _hall_of_fame_monthly.py \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 _hall_of_fame.py \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 _last_game_results.py \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 gin_rummy.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 jinja2/ \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2502 \u2502 \u251c\u2500\u2500 migrations/ \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2502 \u2502 \u251c\u2500\u2500 admin.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 apps.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 helpers.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 http_payloads.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 models.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 urls.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 views.py \u2502 \u2502 \u2514\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 project/ \u2502 \u2502 \u251c\u2500\u2500 settings/ \u2502 \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2502 \u251c\u2500\u2500 asgi.py # (3) \u2502 \u2502 \u251c\u2500\u2500 jinja2.py \u2502 \u2502 \u251c\u2500\u2500 urls.py \u2502 \u2502 \u2514\u2500\u2500 wsgi.py \u2502 \u2514\u2500\u2500 manage.py* \u251c\u2500\u2500 tests/ \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 docker-compose.yml \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 poetry.lock \u2514\u2500\u2500 pyproject.toml In this package we have the code that alters data In this package we have the code that fetches data asgi.py , urls.py and wsgi.py were created by the startproject command. jinja2.py is where I quickly configure Jinja, following the Django documentation: > https://docs.djangoproject.com/en/4.0/topics/templates/#django.template.backends.jinja2.Jinja2 We can see that for each Django app we have 2 sub-packages: domain.mutations is where the code that alter data lives domain.queries is where the code that fetch data lives They're both structured the same way, following these principles from HackSoft's styleguide: Each of their module exposes only one public function - and optionally some types if it has to, in order to describe the shape of its input and/or output. Each of these one-per-module-functions only accepts keyword arguments. These functions' signatures should be fully \"type hinted\". Each module is free to use as many private functions it needs to achieve the job described by its single public function. For the mutations, these functions' name should start with a verb , since they're the reflection of a business logic action Each module's name is prefixed with an underscore, to emphasise that it should not be imported directly The __init__.py file is in charge of exposing the public function of each module to the \"outside world\" - i.e. the Python code that doesn't live in the same package. Info The goal of the last 2 points is to avoid this kind of imports, where we have to repeat the same name twice - once of the module name and once for the function itself: from .domain.mutations.save_game_result import save_game_result So instead of this repetition we can simply do: from .domain.mutations import save_game_result A concrete example, with the Gin Rummy leaderboard app \u00b6 Ok, enough theory - let's see how that works in the context of this mini project! We have a single Django model , that looks like this: class GameResult ( models . Model ): player_north_name = models . CharField ( max_length = 50 ) player_south_name = models . CharField ( max_length = 50 ) outcome = models . CharField ( max_length = 10 , choices = [( outcome , outcome ) for outcome in GAME_OUTCOME . __args__ ]) # type: ignore # These 2 ones can be `null` when the outcome is `draw`: winner_name = models . CharField ( max_length = 50 , null = True ) deadwood_value = models . PositiveSmallIntegerField ( null = True ) # Computed from the previous `outcome` and `deadwood_value` fields: winner_score = models . PositiveSmallIntegerField ( null = True ) created_at = models . DateTimeField ( default = timezone . now ) @property def is_draw ( self ) -> bool : return self . outcome == \"draw\" @cached_property def loser_name ( self ) -> str | None : if self . is_draw : return None return [ name for name in ( self . player_north_name , self . player_south_name ) if name != self . winner_name ][ 0 ] def __str__ ( self ) -> str : return f \" { self . player_north_name . title () } vs { self . player_south_name . title () } , on { self . created_at . strftime ( ' %a %d %b at %H:%M' ) } \" The domain.mutations package of our Django app \u00b6 For this minimalist project we need only one mutation, which is triggered when the user submits the \"New game result\" HTML form: # file: src/apps/gin_scoring/domain/_save_game_result.py from ...domain.gin_rummy import GAME_OUTCOME , calculate_round_score from ...models import GameResult def save_game_result ( * , # (1) player_north_name : str , player_south_name : str , outcome : GAME_OUTCOME , # (2) winner_name : str | None , deadwood_value : int , ) -> GameResult : is_draw = outcome == \"draw\" winner_score = None if is_draw : winner_name = None else : winner_score = calculate_round_score ( game_outcome = outcome , deadwood_value = deadwood_value ) game_result_model = GameResult ( # (3) player_north_name = player_north_name , player_south_name = player_south_name , outcome = outcome , winner_name = winner_name , deadwood_value = deadwood_value , winner_score = winner_score , ) # (4) game_result_model . save () return game_result_model We force this function to be used only with the \"keyword arguments\" syntax GAME_OUTCOME is just a literal type, described later on in this same article To my knowledge there's no equivalent of the Shorthand property names of ES2015 in Python - which I guess must be on purpose, since readability is almost always the top priority of the language? For that reason we have to repeat the [field name]=[arg name] pattern, but in my opinion it's not really an issue The model also has a created_at field, but it will automatically be set by the Django ORM since we've used default=timezone.now when we defined the models.DateTimeField field And then, we expose that function to the rest of the Python code: # file: src/apps/gin_scoring/domain/mutations/__init__.py from ._save_game_result import save_game_result All we have to do now is to use that mutation from a Django view. There are several ways to do this, but here is an example: # file: src/apps/gin_scoring/views.py from .domain.mutations import save_game_result # ... @require_POST def post_game_result ( request : HttpRequest ) -> HttpResponse : try : game_result_payload = GameResultPayload ( ** request . POST . dict ()) except pydantic . ValidationError : return HttpResponseBadRequest () save_game_result ( # (1) player_north_name = game_result_payload . player_north_name , player_south_name = game_result_payload . player_south_name , outcome = game_result_payload . outcome , winner_name = game_result_payload . winner_name , deadwood_value = game_result_payload . deadwood_value , ) return redirect ( \"index\" ) Note that we could also have opted for a more concise way to transfer data from the \"validation and normalisation\" data structure to the mutation: save_game_result ( ** game_result_payload . dict () ) Validating the input of our Django views There are multiple ways to validate and normalise the input of our Django views, before passing its data to the \"domain\" layer. In this case I chose to use Pydantic . # file: src/apps/gin_scoring/http_payloads.py from typing import Any import pydantic from .domain.gin_rummy import GAME_OUTCOME from .helpers import normalize_player_name class GameResultPayload ( pydantic . BaseModel ): player_north_name : str player_south_name : str outcome : GAME_OUTCOME winner_name : str | None deadwood_value : int @pydantic . root_validator ( pre = True ) def normalize_player_names ( cls , values : dict [ str , Any ]): # In order to have consistent recording when players \"Rachel\" and \"Olivier\" add a game result, whether # \"Rachel\" is \"north\" and \"Olivier\" is \"south\" or vice-versa, we sort their names alphabetically # and then always set the \"north\" player to the first one and the \"south\" one to the second one: player_north_name , player_south_name = sorted ( ( normalize_player_name ( values [ \"player_north_name\" ]), normalize_player_name ( values [ \"player_south_name\" ]), ) ) values [ \"player_north_name\" ] = player_north_name values [ \"player_south_name\" ] = player_south_name return values @pydantic . validator ( \"winner_name\" ) def validate_winner_name ( cls , v : str , values : dict [ str , Any ]) -> str | None : is_draw = values [ \"outcome\" ] == \"draw\" if is_draw : return None # No winner name for \"draw\" games if not v : raise ValueError ( f \"non-draw games must have a winner name\" ) winner_name = normalize_player_name ( v ) player_names = ( values [ \"player_north_name\" ], values [ \"player_south_name\" ]) if winner_name not in player_names : raise ValueError ( f \"winner name { v } is not part of the players' names ' { ',' . join ( player_names ) } '\" ) return winner_name The domain.queries package of our Django app \u00b6 For this simple app we need only 3 queries: One to get the global \"Hall of fame\", where we determine the ranking of players based on all the games played so far One to get the monthly \"Hall of fame\", which does the same but with a distinct ranking for each month One that returns exhaustive data for the last 10 games that were played Let's take a look at the second one, for example: # file: src/apps/gin_scoring/domain/queries/_hall_of_fame_monthly.py from collections import defaultdict from datetime import datetime from typing import NamedTuple from django.db.models import Count , Sum from django.db.models.functions import TruncMonth from ...models import GameResult # (1) class HallOfFameMonthResult ( NamedTuple ): # (2) month : datetime winner_name : str game_counts : int win_counts : int score_delta : int def hall_of_fame_monthly () -> list [ HallOfFameMonthResult ]: # \u26a0\ufe0f Probably not the very best way to achieve this... # But this is a project I gave myself one single day to build, # so that will do the job \ud83d\ude05 # @link https://docs.djangoproject.com/en/4.0/topics/db/aggregation/ win_counts = Count ( \"winner_score\" ) total_score = Sum ( \"winner_score\" ) raw_results = ( GameResult . objects . filter ( winner_name__isnull = False ) . annotate ( month = TruncMonth ( \"created_at\" )) # (3) . values ( \"month\" , \"winner_name\" ) . distinct () . annotate ( win_counts = win_counts , total_score = total_score ) # Each won round is worth 25 points: . annotate ( grand_total = ( win_counts * 25 ) + total_score ) . order_by ( \"-month\" , \"-grand_total\" ) ) raw_results_per_month : dict [ datetime , list [ dict ]] = defaultdict ( list ) for raw_result in raw_results : raw_results_per_month [ raw_result [ \"month\" ]] . append ( raw_result ) returned_results : list [ HallOfFameMonthResult ] = [] for month , month_results in raw_results_per_month . items (): winner_result = month_results [ 0 ] winner_grand_total = winner_result [ \"grand_total\" ] or 0 second_best_grand_total = 0 if len ( month_results ) < 2 else ( month_results [ 1 ][ \"grand_total\" ] or 0 ) games_count = sum ([ res [ \"win_counts\" ] for res in month_results ]) returned_results . append ( HallOfFameMonthResult ( month = month , winner_name = winner_result [ \"winner_name\" ], game_counts = games_count , win_counts = winner_result [ \"win_counts\" ], score_delta = winner_grand_total - second_best_grand_total , ) ) return returned_results I like using relative imports for things we import that live in the same Django app . In all other cases I'd use absolute imports. We're not returning ActiveRecord items directly from a Django QuerySet there, but aggregated results. There are several data structures we can use in Python for that kind of \"value objects\", but I generally opt for typing.NamedTuple This is where we group the database rows by month Yes, I could probably have used itertools.groupby instead ^_^ Note Note that we could have split this into several functions - in which case they would all be private functions (their name would start with an underscore), and only the \"domain\" one would be public And similarly, the __init__.py file is in charge of exposing only what the rest of the Python # file: src/apps/gin_scoring/domain/queries/__init__.py from ._hall_of_fame import hall_of_fame from ._hall_of_fame_monthly import ( HallOfFameMonthResult , # (1) hall_of_fame_monthly ) from ._last_game_results import last_game_results # (2) Sometimes it's useful to not only export the one public function of the module, but also a dedicated type it's using for its input or output - so other Python modules can also use type hints when interacting with the \"domain\" layer But most of the time, all we need is to expose the public function of the module But what about the domain logic that is neither a mutation nor a query? \u00b6 We still have to put somewhere some parts of the domain don't fall in either categories. For example: Constants, enums, literal types... Data structures describing some aspects of the business logic, that can be used by both mutations and queries. Various forms of \"memory-only\" computations. Well... In my case, I find that in each Django app the domain package itself is a very good place to welcome these! For this mini project, for example, I chose to have a single Python module ( apps.gin_scoring.domain.gin_rummy ) to store some \"business-logic-related\" stuff that is specific to the Gin Rummy game , and that are neither a mutation nor a query: # file: src/apps/gin_scoring/domain/gin_rummy.py from typing import Literal # Possible outcomes of a Gin Rummy game: # (1) GAME_OUTCOME = Literal [ \"knock\" , \"gin\" , \"big_gin\" , \"undercut\" , \"draw\" ] def calculate_round_score ( * , game_outcome : GAME_OUTCOME , deadwood_value : int ) -> int : # @link https://en.wikipedia.org/wiki/Gin_rummy#Knocking match game_outcome : case \"draw\" : return 0 case \"knock\" : return deadwood_value case \"gin\" : return 25 + deadwood_value case \"big_gin\" : return 31 + deadwood_value case \"undercut\" : return 15 + deadwood_value case _ : raise ValueError ( f \"Invalid game outcome value ' { game_outcome } '\" ) More on that Literal type below We can see indeed that calculating the score of a round, depending on its outcome and the value of its deadwood , is neither a mutation nor a query: it's just a standalone computation, that does not depend on anything we would have in a database. And the same goes for enumerating the possible outcomes of a Gin Rummy game. Literals or Enums? To express the outcome of a Gin Rummy game I could of course have used a Python enum instead: @enum . unique class GameOutcome ( enum . Enum ): KNOCK = \"knock\" # or `enum.auto()` GIN = \"gin\" BIG_GIN = \"big_gin\" UNDERCUT = \"undercut\" DRAW = \"draw\" As for me, I must admit that I have no strict rules when I have to choose between one or another way to describe that kind of data I was mostly using Enums until a few years ago, but as I was using TypeScript more and more I realised that I really liked using literal types there - the TypeScript equivalent of that Python Literal would be: export type GAME_OUTCOME = \"knock\" | \"gin\" | \"big_gin\" | \"undercut\" | \"draw\" I appreciate the concision of literals, and tend to use them when I have the feeling that having such \"literal values\" spread in the code wouldn't cause any issue later on if I have to change their values Enums are certainly easier to handle in case of refactorings, but so far I've never come across a case where during a refactoring I regretted having opted for a literal rather than an Enum - fingers crossed, it won't be the case anytime soon! And that's it! \u00b6 As we can see the pattern is very simple to implement, and its few principles are a very good guideline for developers when they have to add some code. Is it code that creates, updates or deletes data in a database? Let's create a new module in the domain.mutations package of the related Django app, that will expose one single \"kwargs-only\" function - its name will start with a verb. Is it code that reads data from a database? Let's create a new module in the domain.queries package of the related Django app, that will expose one single \"kwargs-only\" function. Is it code that expresses the business logic but neither alters nor reads data from a database? Let's put that in a module of the domain package of the related Django app. The beauty of that pattern is that it really scales very well, despite its simplicity: my former teams and I used it for years on ever-growing code bases without having ever faced a case where the pattern would show a limitation. Info The 3rd (and last) article of this series will be a quick one, about how I hosted this app - for free - at the end of that single day of work. I might also share a bit about the \"code quality\" tools I've used, in case it could be useful to anyone Acknowledgements \u00b6 Thank you so much HackSoft for your Django Styleguide ! I would also like to thank Audrey and Daniel Roy Greenfeld for their book Two Scoops Of Django , which was a very helpful resource for me when I started to learn Django and tried to see what the best practices could be in this ecosystem - definitely worth the purchase! And thanks again to my friend Yann - einenlum.com - for his careful review and useful feedback on this article","title":"'From scratch to online in production' in a single day, with Django - Part 2"},{"location":"2022/07-22---from-scratch-to-online-in-production-in-a-single-day-with-django-part-2/#from-scratch-to-online-in-production-in-a-single-day-with-django-part-2","text":"","title":"'From scratch to online in production' in a single day, with Django - Part 2"},{"location":"2022/07-22---from-scratch-to-online-in-production-in-a-single-day-with-django-part-2/#quick-summary-of-the-previous-part","text":"In part 1 we saw a way to set up a Django project in a quite generic way, that can be used for any kind of project: Dependencies are managed by Poetry Settings that differ from an environment to another come from environment variables, and are populated from optional .env files in \"local development\" mode, powered by django-environ . SECRET_KEY = env . str ( \"SECRET_KEY\" ) DATABASES = { \"default\" : env . db_url ( \"DATABASE_URL\" ), } On top of this we also have a Python module for each type of environment - so we'll use the DJANGO_SETTINGS_MODULE environment variable to point to project.settings.development during local development , project.settings.heroku on Heroku, project.settings.test when running our test suite, etc. Info Having such multiple settings files is a best practice I learned by reading the great book Two Scoops of Django - which itself cites this talk from Jacob Kaplan-Moss. A src/ folder contains all the application code, within 2 top-level packages for our Python modules: project , which contains the Django settings and the WSGI/ASGI HTTP entrypoints apps for the Django apps - so each of them is free to have any name we want, without any risks of collisions with an existing 3rd-party package.","title":"Quick summary of the previous part"},{"location":"2022/07-22---from-scratch-to-online-in-production-in-a-single-day-with-django-part-2/#next-step-we-have-to-organise-the-business-logic-inside-our-django-apps","text":"Django has some built-in recommendations about how to structure a project. Its applications concept for example is a good start to split the code into smaller units, and some basic rules like this one are good guidelines: Quote If there are 20+ models in a single app, think about ways to break it down into smaller apps, as it probably means your app is doing too much. In practice, we like to lower this number to no more than five to ten models per app . From Two Scoops of Django Now, let's take a look at an aspect that may be a mystery for other developers who enter this new ecosystem they're not familiar with yet - like me a few years ago, when I switched from \"PHP + Symfony \" to \"Python + Django \" : How to organise the business logic inside each Django app? Me, as I was learning Django Well, I guess \"the Django way\" would probably be to follow the Active Record pattern and write most of the code for this in the Models themselves - as well as in their Managers (which are similar to \"repositories\" or \"entity managers\" when using the Data Mapper pattern) However, in my own (subjective) case I generally find that doing so doesn't scale very well as the project grows, as we end up having all the business logic grouped in a bunch of huge classes. Which is why, as I was learning Django, I was quite eager to find another way to structure my code...","title":"Next step: we have to organise the business logic inside our Django apps"},{"location":"2022/07-22---from-scratch-to-online-in-production-in-a-single-day-with-django-part-2/#an-efficient-pattern-for-the-business-logic-simplicity-that-scales-well","text":"When I started to learn Django I was lucky enough to stumble upon this Django Styleguide , published on GitHub by the software development company HackSoft. And more specifically this part, where they explain \"services\" and \"selectors\": https://github.com/HackSoftware/Django-Styleguide#services","title":"An efficient pattern for the business logic: simplicity that scales well"},{"location":"2022/07-22---from-scratch-to-online-in-production-in-a-single-day-with-django-part-2/#my-own-adaptation-of-the-services-and-selectors-concept-from-hacksoft","text":"As I'm mostly working with GraphQL in my day-to-day job, I opted for a terminology that rings a bell a bit more to my ears than \"services\" and \"selectors\" : mutations and queries . The former is a package that contains code that alter a database (adding, modifying or deleting data from it), while the latter is specialised in fetching data. Here is how it looks like applied to my \"Gin Rummy leaderboard\" mini project: (the parts of the tree that don't matter in this case are replaced with three dots) gin-scoring/ \u251c\u2500\u2500 src/ \u2502 \u251c\u2500\u2500 apps/ \u2502 \u2502 \u251c\u2500\u2500 authentication/ \u2502 \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2502 \u251c\u2500\u2500 gin_scoring/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 domain/ \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 mutations/ # (1) \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 _save_game_result.py \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 queries/ # (2) \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 _hall_of_fame_monthly.py \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 _hall_of_fame.py \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 _last_game_results.py \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 gin_rummy.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 jinja2/ \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2502 \u2502 \u251c\u2500\u2500 migrations/ \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2502 \u2502 \u251c\u2500\u2500 admin.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 apps.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 helpers.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 http_payloads.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 models.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 urls.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 views.py \u2502 \u2502 \u2514\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 project/ \u2502 \u2502 \u251c\u2500\u2500 settings/ \u2502 \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2502 \u251c\u2500\u2500 asgi.py # (3) \u2502 \u2502 \u251c\u2500\u2500 jinja2.py \u2502 \u2502 \u251c\u2500\u2500 urls.py \u2502 \u2502 \u2514\u2500\u2500 wsgi.py \u2502 \u2514\u2500\u2500 manage.py* \u251c\u2500\u2500 tests/ \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 docker-compose.yml \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 poetry.lock \u2514\u2500\u2500 pyproject.toml In this package we have the code that alters data In this package we have the code that fetches data asgi.py , urls.py and wsgi.py were created by the startproject command. jinja2.py is where I quickly configure Jinja, following the Django documentation: > https://docs.djangoproject.com/en/4.0/topics/templates/#django.template.backends.jinja2.Jinja2 We can see that for each Django app we have 2 sub-packages: domain.mutations is where the code that alter data lives domain.queries is where the code that fetch data lives They're both structured the same way, following these principles from HackSoft's styleguide: Each of their module exposes only one public function - and optionally some types if it has to, in order to describe the shape of its input and/or output. Each of these one-per-module-functions only accepts keyword arguments. These functions' signatures should be fully \"type hinted\". Each module is free to use as many private functions it needs to achieve the job described by its single public function. For the mutations, these functions' name should start with a verb , since they're the reflection of a business logic action Each module's name is prefixed with an underscore, to emphasise that it should not be imported directly The __init__.py file is in charge of exposing the public function of each module to the \"outside world\" - i.e. the Python code that doesn't live in the same package. Info The goal of the last 2 points is to avoid this kind of imports, where we have to repeat the same name twice - once of the module name and once for the function itself: from .domain.mutations.save_game_result import save_game_result So instead of this repetition we can simply do: from .domain.mutations import save_game_result","title":"My own adaptation of the \"services\" and \"selectors\" concept from HackSoft"},{"location":"2022/07-22---from-scratch-to-online-in-production-in-a-single-day-with-django-part-2/#a-concrete-example-with-the-gin-rummy-leaderboard-app","text":"Ok, enough theory - let's see how that works in the context of this mini project! We have a single Django model , that looks like this: class GameResult ( models . Model ): player_north_name = models . CharField ( max_length = 50 ) player_south_name = models . CharField ( max_length = 50 ) outcome = models . CharField ( max_length = 10 , choices = [( outcome , outcome ) for outcome in GAME_OUTCOME . __args__ ]) # type: ignore # These 2 ones can be `null` when the outcome is `draw`: winner_name = models . CharField ( max_length = 50 , null = True ) deadwood_value = models . PositiveSmallIntegerField ( null = True ) # Computed from the previous `outcome` and `deadwood_value` fields: winner_score = models . PositiveSmallIntegerField ( null = True ) created_at = models . DateTimeField ( default = timezone . now ) @property def is_draw ( self ) -> bool : return self . outcome == \"draw\" @cached_property def loser_name ( self ) -> str | None : if self . is_draw : return None return [ name for name in ( self . player_north_name , self . player_south_name ) if name != self . winner_name ][ 0 ] def __str__ ( self ) -> str : return f \" { self . player_north_name . title () } vs { self . player_south_name . title () } , on { self . created_at . strftime ( ' %a %d %b at %H:%M' ) } \"","title":"A concrete example, with the Gin Rummy leaderboard app"},{"location":"2022/07-22---from-scratch-to-online-in-production-in-a-single-day-with-django-part-2/#the-domainmutations-package-of-our-django-app","text":"For this minimalist project we need only one mutation, which is triggered when the user submits the \"New game result\" HTML form: # file: src/apps/gin_scoring/domain/_save_game_result.py from ...domain.gin_rummy import GAME_OUTCOME , calculate_round_score from ...models import GameResult def save_game_result ( * , # (1) player_north_name : str , player_south_name : str , outcome : GAME_OUTCOME , # (2) winner_name : str | None , deadwood_value : int , ) -> GameResult : is_draw = outcome == \"draw\" winner_score = None if is_draw : winner_name = None else : winner_score = calculate_round_score ( game_outcome = outcome , deadwood_value = deadwood_value ) game_result_model = GameResult ( # (3) player_north_name = player_north_name , player_south_name = player_south_name , outcome = outcome , winner_name = winner_name , deadwood_value = deadwood_value , winner_score = winner_score , ) # (4) game_result_model . save () return game_result_model We force this function to be used only with the \"keyword arguments\" syntax GAME_OUTCOME is just a literal type, described later on in this same article To my knowledge there's no equivalent of the Shorthand property names of ES2015 in Python - which I guess must be on purpose, since readability is almost always the top priority of the language? For that reason we have to repeat the [field name]=[arg name] pattern, but in my opinion it's not really an issue The model also has a created_at field, but it will automatically be set by the Django ORM since we've used default=timezone.now when we defined the models.DateTimeField field And then, we expose that function to the rest of the Python code: # file: src/apps/gin_scoring/domain/mutations/__init__.py from ._save_game_result import save_game_result All we have to do now is to use that mutation from a Django view. There are several ways to do this, but here is an example: # file: src/apps/gin_scoring/views.py from .domain.mutations import save_game_result # ... @require_POST def post_game_result ( request : HttpRequest ) -> HttpResponse : try : game_result_payload = GameResultPayload ( ** request . POST . dict ()) except pydantic . ValidationError : return HttpResponseBadRequest () save_game_result ( # (1) player_north_name = game_result_payload . player_north_name , player_south_name = game_result_payload . player_south_name , outcome = game_result_payload . outcome , winner_name = game_result_payload . winner_name , deadwood_value = game_result_payload . deadwood_value , ) return redirect ( \"index\" ) Note that we could also have opted for a more concise way to transfer data from the \"validation and normalisation\" data structure to the mutation: save_game_result ( ** game_result_payload . dict () ) Validating the input of our Django views There are multiple ways to validate and normalise the input of our Django views, before passing its data to the \"domain\" layer. In this case I chose to use Pydantic . # file: src/apps/gin_scoring/http_payloads.py from typing import Any import pydantic from .domain.gin_rummy import GAME_OUTCOME from .helpers import normalize_player_name class GameResultPayload ( pydantic . BaseModel ): player_north_name : str player_south_name : str outcome : GAME_OUTCOME winner_name : str | None deadwood_value : int @pydantic . root_validator ( pre = True ) def normalize_player_names ( cls , values : dict [ str , Any ]): # In order to have consistent recording when players \"Rachel\" and \"Olivier\" add a game result, whether # \"Rachel\" is \"north\" and \"Olivier\" is \"south\" or vice-versa, we sort their names alphabetically # and then always set the \"north\" player to the first one and the \"south\" one to the second one: player_north_name , player_south_name = sorted ( ( normalize_player_name ( values [ \"player_north_name\" ]), normalize_player_name ( values [ \"player_south_name\" ]), ) ) values [ \"player_north_name\" ] = player_north_name values [ \"player_south_name\" ] = player_south_name return values @pydantic . validator ( \"winner_name\" ) def validate_winner_name ( cls , v : str , values : dict [ str , Any ]) -> str | None : is_draw = values [ \"outcome\" ] == \"draw\" if is_draw : return None # No winner name for \"draw\" games if not v : raise ValueError ( f \"non-draw games must have a winner name\" ) winner_name = normalize_player_name ( v ) player_names = ( values [ \"player_north_name\" ], values [ \"player_south_name\" ]) if winner_name not in player_names : raise ValueError ( f \"winner name { v } is not part of the players' names ' { ',' . join ( player_names ) } '\" ) return winner_name","title":"The domain.mutations package of our Django app"},{"location":"2022/07-22---from-scratch-to-online-in-production-in-a-single-day-with-django-part-2/#the-domainqueries-package-of-our-django-app","text":"For this simple app we need only 3 queries: One to get the global \"Hall of fame\", where we determine the ranking of players based on all the games played so far One to get the monthly \"Hall of fame\", which does the same but with a distinct ranking for each month One that returns exhaustive data for the last 10 games that were played Let's take a look at the second one, for example: # file: src/apps/gin_scoring/domain/queries/_hall_of_fame_monthly.py from collections import defaultdict from datetime import datetime from typing import NamedTuple from django.db.models import Count , Sum from django.db.models.functions import TruncMonth from ...models import GameResult # (1) class HallOfFameMonthResult ( NamedTuple ): # (2) month : datetime winner_name : str game_counts : int win_counts : int score_delta : int def hall_of_fame_monthly () -> list [ HallOfFameMonthResult ]: # \u26a0\ufe0f Probably not the very best way to achieve this... # But this is a project I gave myself one single day to build, # so that will do the job \ud83d\ude05 # @link https://docs.djangoproject.com/en/4.0/topics/db/aggregation/ win_counts = Count ( \"winner_score\" ) total_score = Sum ( \"winner_score\" ) raw_results = ( GameResult . objects . filter ( winner_name__isnull = False ) . annotate ( month = TruncMonth ( \"created_at\" )) # (3) . values ( \"month\" , \"winner_name\" ) . distinct () . annotate ( win_counts = win_counts , total_score = total_score ) # Each won round is worth 25 points: . annotate ( grand_total = ( win_counts * 25 ) + total_score ) . order_by ( \"-month\" , \"-grand_total\" ) ) raw_results_per_month : dict [ datetime , list [ dict ]] = defaultdict ( list ) for raw_result in raw_results : raw_results_per_month [ raw_result [ \"month\" ]] . append ( raw_result ) returned_results : list [ HallOfFameMonthResult ] = [] for month , month_results in raw_results_per_month . items (): winner_result = month_results [ 0 ] winner_grand_total = winner_result [ \"grand_total\" ] or 0 second_best_grand_total = 0 if len ( month_results ) < 2 else ( month_results [ 1 ][ \"grand_total\" ] or 0 ) games_count = sum ([ res [ \"win_counts\" ] for res in month_results ]) returned_results . append ( HallOfFameMonthResult ( month = month , winner_name = winner_result [ \"winner_name\" ], game_counts = games_count , win_counts = winner_result [ \"win_counts\" ], score_delta = winner_grand_total - second_best_grand_total , ) ) return returned_results I like using relative imports for things we import that live in the same Django app . In all other cases I'd use absolute imports. We're not returning ActiveRecord items directly from a Django QuerySet there, but aggregated results. There are several data structures we can use in Python for that kind of \"value objects\", but I generally opt for typing.NamedTuple This is where we group the database rows by month Yes, I could probably have used itertools.groupby instead ^_^ Note Note that we could have split this into several functions - in which case they would all be private functions (their name would start with an underscore), and only the \"domain\" one would be public And similarly, the __init__.py file is in charge of exposing only what the rest of the Python # file: src/apps/gin_scoring/domain/queries/__init__.py from ._hall_of_fame import hall_of_fame from ._hall_of_fame_monthly import ( HallOfFameMonthResult , # (1) hall_of_fame_monthly ) from ._last_game_results import last_game_results # (2) Sometimes it's useful to not only export the one public function of the module, but also a dedicated type it's using for its input or output - so other Python modules can also use type hints when interacting with the \"domain\" layer But most of the time, all we need is to expose the public function of the module","title":"The domain.queries package of our Django app"},{"location":"2022/07-22---from-scratch-to-online-in-production-in-a-single-day-with-django-part-2/#but-what-about-the-domain-logic-that-is-neither-a-mutation-nor-a-query","text":"We still have to put somewhere some parts of the domain don't fall in either categories. For example: Constants, enums, literal types... Data structures describing some aspects of the business logic, that can be used by both mutations and queries. Various forms of \"memory-only\" computations. Well... In my case, I find that in each Django app the domain package itself is a very good place to welcome these! For this mini project, for example, I chose to have a single Python module ( apps.gin_scoring.domain.gin_rummy ) to store some \"business-logic-related\" stuff that is specific to the Gin Rummy game , and that are neither a mutation nor a query: # file: src/apps/gin_scoring/domain/gin_rummy.py from typing import Literal # Possible outcomes of a Gin Rummy game: # (1) GAME_OUTCOME = Literal [ \"knock\" , \"gin\" , \"big_gin\" , \"undercut\" , \"draw\" ] def calculate_round_score ( * , game_outcome : GAME_OUTCOME , deadwood_value : int ) -> int : # @link https://en.wikipedia.org/wiki/Gin_rummy#Knocking match game_outcome : case \"draw\" : return 0 case \"knock\" : return deadwood_value case \"gin\" : return 25 + deadwood_value case \"big_gin\" : return 31 + deadwood_value case \"undercut\" : return 15 + deadwood_value case _ : raise ValueError ( f \"Invalid game outcome value ' { game_outcome } '\" ) More on that Literal type below We can see indeed that calculating the score of a round, depending on its outcome and the value of its deadwood , is neither a mutation nor a query: it's just a standalone computation, that does not depend on anything we would have in a database. And the same goes for enumerating the possible outcomes of a Gin Rummy game. Literals or Enums? To express the outcome of a Gin Rummy game I could of course have used a Python enum instead: @enum . unique class GameOutcome ( enum . Enum ): KNOCK = \"knock\" # or `enum.auto()` GIN = \"gin\" BIG_GIN = \"big_gin\" UNDERCUT = \"undercut\" DRAW = \"draw\" As for me, I must admit that I have no strict rules when I have to choose between one or another way to describe that kind of data I was mostly using Enums until a few years ago, but as I was using TypeScript more and more I realised that I really liked using literal types there - the TypeScript equivalent of that Python Literal would be: export type GAME_OUTCOME = \"knock\" | \"gin\" | \"big_gin\" | \"undercut\" | \"draw\" I appreciate the concision of literals, and tend to use them when I have the feeling that having such \"literal values\" spread in the code wouldn't cause any issue later on if I have to change their values Enums are certainly easier to handle in case of refactorings, but so far I've never come across a case where during a refactoring I regretted having opted for a literal rather than an Enum - fingers crossed, it won't be the case anytime soon!","title":"But what about the domain logic that is neither a mutation nor a query?"},{"location":"2022/07-22---from-scratch-to-online-in-production-in-a-single-day-with-django-part-2/#and-thats-it","text":"As we can see the pattern is very simple to implement, and its few principles are a very good guideline for developers when they have to add some code. Is it code that creates, updates or deletes data in a database? Let's create a new module in the domain.mutations package of the related Django app, that will expose one single \"kwargs-only\" function - its name will start with a verb. Is it code that reads data from a database? Let's create a new module in the domain.queries package of the related Django app, that will expose one single \"kwargs-only\" function. Is it code that expresses the business logic but neither alters nor reads data from a database? Let's put that in a module of the domain package of the related Django app. The beauty of that pattern is that it really scales very well, despite its simplicity: my former teams and I used it for years on ever-growing code bases without having ever faced a case where the pattern would show a limitation. Info The 3rd (and last) article of this series will be a quick one, about how I hosted this app - for free - at the end of that single day of work. I might also share a bit about the \"code quality\" tools I've used, in case it could be useful to anyone","title":"And that's it!"},{"location":"2022/07-22---from-scratch-to-online-in-production-in-a-single-day-with-django-part-2/#acknowledgements","text":"Thank you so much HackSoft for your Django Styleguide ! I would also like to thank Audrey and Daniel Roy Greenfeld for their book Two Scoops Of Django , which was a very helpful resource for me when I started to learn Django and tried to see what the best practices could be in this ecosystem - definitely worth the purchase! And thanks again to my friend Yann - einenlum.com - for his careful review and useful feedback on this article","title":"Acknowledgements"},{"location":"2022/07-26---triggering-a-github-action-from-an-external-source/","tags":["TIL","github"],"text":"Triggering a GitHub Action from an external source \u00b6 Abstract TIL : a GitHub Action on a given repository can trigger Actions on other GitHub repositories - which is really handy, and enables fancy scenarios of cooperation between repositories! Setting up a self-updating GitHub profile, following the steps shared by Simon Willison \u00b6 Yesterday I've set up a custom GitHub profile, and that README file includes a \"code-generated\" part that automatically displays the last entries from this devblog. It was fun! I've followed the instructions from GitHub themselves, as well as the content generously shared by Simon Willison on his blog about explaining how his own (shiny ) GitHub profile is automatically updated once an hour with quite a lot of dynamic content: https://docs.github.com/en/account-and-profile/setting-up-and-managing-your-github-profile/customizing-your-profile/managing-your-profile-readme https://simonwillison.net/2020/Jul/10/self-updating-profile-readme/ So far so good, by copy-pasting most of his code and adapting it to my own case I got my own GitHub profile also updated automatically once an hour. Note The main difference between Simon Willison's build script and mine is that the dynamic content I have in my own README is much smaller, as I only want to display the last 10 items of this devblog there . As a result the process can be simpler - and even managed only with Python and its standard library! Using the standard library to fetch the RSS feed of this devblog over HTTP and then parse it is a bit less straightforward than what I could have done with higher level packages like Requests and feedparser , but it's still pretty simple The main drawback is probably that Python's xml.etree.ElementTree has a big red warning saying it is \"not secure against maliciously constructed data\" - but in my case what I parse is the RSS feed from my own blog, so it shouldn't be a problem My own \"README update\" script is there: https://github.com/olivierphi/olivierphi/blob/main/build_readme.py Updating the GitHub profile only when the devblog is updated \u00b6 My friend Yann noticed that triggering this README generation once an hour doesn't really make sense, since contrary to Simon Willison the only source of dynamic data in this profile is the devblog: so what would be ideal would be to automatically update the README only when the devblog is updated . Two hours in the GitHub Actions documentation later, I got it working - let's keep a note of how to do this, so I can do it again later on more easily Triggering a GitHub Action from an external source \u00b6 It seems that the only way to trigger a GitHub Action from an event that is not on the GitHub repository itself is to use the repository_dispatch event . In my case the emitter of the event will be a GitHub Action living on this devblog repository, while the receiver will be another GitHub Action, living in the olivierphi repository. (since my GitHub profile is \"olivierphi\", according to the doc my GitHub profile must be a README file living in a GitHub repository that has the sane name) Of course, triggering a GitHub Action in a given repository must not be something that anyone can do, as it would be annoying to have such Actions triggered randomly by 3rd-party people or scripts - and even more annoying, running an Action can leak sensitive information Which is why the emitter of a repository_dispatch event have to authenticate itself . GitHub's Personal Access Tokens are a way to do this. Generating a GitHub Personal Access Token \u00b6 Unfortunately, it seems that Personal Access Tokens have quite a poor granularity: the emitter of a repository_dispatch event must use a token with the repo scope , which gives it \"full access to repositories, including private repositories\" Note I thought I could create such a Token that would be limited to my GitHub profile repository, but it seems that Personal Access Tokens don't have such a level of granularity, so I do have to give such a \"god-like\" access to my devblog repository The steps to generate such a token are documented here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token Storing the Token on the event emitter's side \u00b6 This Token having such great power, I should store it in a safe place... It seems that GitHub Actions' Encrypted Secrets is what I need! https://docs.github.com/en/actions/security-guides/encrypted-secrets Sending the repository_dispatch event when this DevBlog is updated \u00b6 Right, now I have a (overpowered ) Token, and it's safely stored in the devblog repository as an Encrypted Secret... Now all I have to do is to use it to send such an event to the repo that hosts my GitHub profile! There are multiple ways to do this, and I went for the one that was looking the most straightforward to me. So at the end of the GitHub Action file that deploys the blog generated by Material for MKDocs when I push an update to the git repo, I just added the following step : - name : Notify my Github profile repo env : TOKEN : ${{ secrets.MY_TOKEN_SECRET_NAME }} # (1) run : # (2) |- curl \\ --silent \\ -X POST \\ -H \"Accept : application/vnd.github+json\" \\ -H \"Authorization : token ${TOKEN}\" \\ \"https://api.github.com/repos/olivierphi/olivierphi/dispatches\" \\ -d '{\"event_type\":\"devblog-gh-pages-pushed\",\"client_payload\":{\"wait_for_deployment\":true}}' We ask GitHub to extract the encrypted secret to an environment variable that I name \"TOKEN\" The target URL includes olivierphi/olivierphi : the first one is the name of my GitHub profile, while the second one is the name of the GitHub repository. They have to be identical in the case of the GitHub profile. I copy-pasted this curl command from there: https://docs.github.com/en/rest/repos/repos#create-a-repository-dispatch-event Receiving the repository_dispatch event on the GitHub profile repo \u00b6 I already have a GitHub Action file that is in charge of re-building dynamically the content of my GitHUb profile's README when I push some content. All I have to do now is to remove the hourly build, and subscribe to the repository_dispatch event: on : push : workflow_dispatch : # (1) # This is removed: schedule : - cron : '33 * * * *' # rebuilt once an hour at xx:33 # This is added: repository_dispatch : # triggered by my \"devblog\" repo when something is pushed on the GH Pages branch types : [ devblog-gh-pages-pushed ] Thanks to this workflow_dispatch we can also manually trigger the GitHub Action . Conditionally waiting for the DevBlog's deployment \u00b6 There is one last thing I have to manage: at the time when this GitHub Action is triggered, the DevBlog's static content was just pushed to the gh-pages branch by MKDocs, so the updated DevBlog is not online yet ! So just doing this is not enough, and we need to wait for the RSS feed to be up-to-date before re-generating the README of the GitHub profile... It seems that most of the time this deployment takes about 30 to 40 seconds, but sometimes it took a little more than a minute; right, let's wait for 90 seconds before reading the RSS feed, and it should do the job. However, I don't want to have this waiting time when I push an updated version of the README file itself, or when I trigger the GitHub Action manually! 15 browser tabs opened on the GH Actions documentation later, I found a solution - name : Wait for devblog deployment on GitHub Pages if : ${{ github.event_name == 'repository_dispatch' && github.event.client_payload.wait_for_deployment }} run : |- echo \"Deployment can take up to 1 minute, let's wait for 90 seconds\" sleep 90 - name : Update README run : |- python build_readme.py cat README.md So the Action will sleep for 90 seconds before running the Python script that fetches the blog's content and update the README accordingly, but only if : The Action was triggered by a \"repository_dispatch\" event. We can detect this with: github.event_name == 'repository_dispatch' The emitter sent a parameter - that I arbitrarily called wait_for_deployment - in its JSON payload, with a truthy value (which is what I did in the curl command earlier ). We can detect this with: github.event.client_payload.wait_for_deployment Job's done! \u00b6 Now when I push to this DevBlog repo it sends a repository_dispatch event to the repository of my GitHub profile, which will trigger the generation of the README after having waited a bit to let some time for the up-to-date RSS feed to be online In the end, all this happens between these 2 YAML files: https://github.com/olivierphi/olivierphi/blob/main/.github/workflows/build.yml https://github.com/olivierphi/devblog/blob/main/.github/workflows/ci.yml","title":"Triggering a GitHub Action from an external source"},{"location":"2022/07-26---triggering-a-github-action-from-an-external-source/#triggering-a-github-action-from-an-external-source","text":"Abstract TIL : a GitHub Action on a given repository can trigger Actions on other GitHub repositories - which is really handy, and enables fancy scenarios of cooperation between repositories!","title":"Triggering a GitHub Action from an external source"},{"location":"2022/07-26---triggering-a-github-action-from-an-external-source/#setting-up-a-self-updating-github-profile-following-the-steps-shared-by-simon-willison","text":"Yesterday I've set up a custom GitHub profile, and that README file includes a \"code-generated\" part that automatically displays the last entries from this devblog. It was fun! I've followed the instructions from GitHub themselves, as well as the content generously shared by Simon Willison on his blog about explaining how his own (shiny ) GitHub profile is automatically updated once an hour with quite a lot of dynamic content: https://docs.github.com/en/account-and-profile/setting-up-and-managing-your-github-profile/customizing-your-profile/managing-your-profile-readme https://simonwillison.net/2020/Jul/10/self-updating-profile-readme/ So far so good, by copy-pasting most of his code and adapting it to my own case I got my own GitHub profile also updated automatically once an hour. Note The main difference between Simon Willison's build script and mine is that the dynamic content I have in my own README is much smaller, as I only want to display the last 10 items of this devblog there . As a result the process can be simpler - and even managed only with Python and its standard library! Using the standard library to fetch the RSS feed of this devblog over HTTP and then parse it is a bit less straightforward than what I could have done with higher level packages like Requests and feedparser , but it's still pretty simple The main drawback is probably that Python's xml.etree.ElementTree has a big red warning saying it is \"not secure against maliciously constructed data\" - but in my case what I parse is the RSS feed from my own blog, so it shouldn't be a problem My own \"README update\" script is there: https://github.com/olivierphi/olivierphi/blob/main/build_readme.py","title":"Setting up a self-updating GitHub profile, following the steps shared by Simon Willison"},{"location":"2022/07-26---triggering-a-github-action-from-an-external-source/#updating-the-github-profile-only-when-the-devblog-is-updated","text":"My friend Yann noticed that triggering this README generation once an hour doesn't really make sense, since contrary to Simon Willison the only source of dynamic data in this profile is the devblog: so what would be ideal would be to automatically update the README only when the devblog is updated . Two hours in the GitHub Actions documentation later, I got it working - let's keep a note of how to do this, so I can do it again later on more easily","title":"Updating the GitHub profile only when the devblog is updated"},{"location":"2022/07-26---triggering-a-github-action-from-an-external-source/#triggering-a-github-action-from-an-external-source_1","text":"It seems that the only way to trigger a GitHub Action from an event that is not on the GitHub repository itself is to use the repository_dispatch event . In my case the emitter of the event will be a GitHub Action living on this devblog repository, while the receiver will be another GitHub Action, living in the olivierphi repository. (since my GitHub profile is \"olivierphi\", according to the doc my GitHub profile must be a README file living in a GitHub repository that has the sane name) Of course, triggering a GitHub Action in a given repository must not be something that anyone can do, as it would be annoying to have such Actions triggered randomly by 3rd-party people or scripts - and even more annoying, running an Action can leak sensitive information Which is why the emitter of a repository_dispatch event have to authenticate itself . GitHub's Personal Access Tokens are a way to do this.","title":"Triggering a GitHub Action from an external source"},{"location":"2022/07-26---triggering-a-github-action-from-an-external-source/#generating-a-github-personal-access-token","text":"Unfortunately, it seems that Personal Access Tokens have quite a poor granularity: the emitter of a repository_dispatch event must use a token with the repo scope , which gives it \"full access to repositories, including private repositories\" Note I thought I could create such a Token that would be limited to my GitHub profile repository, but it seems that Personal Access Tokens don't have such a level of granularity, so I do have to give such a \"god-like\" access to my devblog repository The steps to generate such a token are documented here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token","title":"Generating a GitHub Personal Access Token"},{"location":"2022/07-26---triggering-a-github-action-from-an-external-source/#storing-the-token-on-the-event-emitters-side","text":"This Token having such great power, I should store it in a safe place... It seems that GitHub Actions' Encrypted Secrets is what I need! https://docs.github.com/en/actions/security-guides/encrypted-secrets","title":"Storing the Token on the event emitter's side"},{"location":"2022/07-26---triggering-a-github-action-from-an-external-source/#sending-the-repository_dispatch-event-when-this-devblog-is-updated","text":"Right, now I have a (overpowered ) Token, and it's safely stored in the devblog repository as an Encrypted Secret... Now all I have to do is to use it to send such an event to the repo that hosts my GitHub profile! There are multiple ways to do this, and I went for the one that was looking the most straightforward to me. So at the end of the GitHub Action file that deploys the blog generated by Material for MKDocs when I push an update to the git repo, I just added the following step : - name : Notify my Github profile repo env : TOKEN : ${{ secrets.MY_TOKEN_SECRET_NAME }} # (1) run : # (2) |- curl \\ --silent \\ -X POST \\ -H \"Accept : application/vnd.github+json\" \\ -H \"Authorization : token ${TOKEN}\" \\ \"https://api.github.com/repos/olivierphi/olivierphi/dispatches\" \\ -d '{\"event_type\":\"devblog-gh-pages-pushed\",\"client_payload\":{\"wait_for_deployment\":true}}' We ask GitHub to extract the encrypted secret to an environment variable that I name \"TOKEN\" The target URL includes olivierphi/olivierphi : the first one is the name of my GitHub profile, while the second one is the name of the GitHub repository. They have to be identical in the case of the GitHub profile. I copy-pasted this curl command from there: https://docs.github.com/en/rest/repos/repos#create-a-repository-dispatch-event","title":"Sending the repository_dispatch event when this DevBlog is updated"},{"location":"2022/07-26---triggering-a-github-action-from-an-external-source/#receiving-the-repository_dispatch-event-on-the-github-profile-repo","text":"I already have a GitHub Action file that is in charge of re-building dynamically the content of my GitHUb profile's README when I push some content. All I have to do now is to remove the hourly build, and subscribe to the repository_dispatch event: on : push : workflow_dispatch : # (1) # This is removed: schedule : - cron : '33 * * * *' # rebuilt once an hour at xx:33 # This is added: repository_dispatch : # triggered by my \"devblog\" repo when something is pushed on the GH Pages branch types : [ devblog-gh-pages-pushed ] Thanks to this workflow_dispatch we can also manually trigger the GitHub Action .","title":"Receiving the repository_dispatch event on the GitHub profile repo"},{"location":"2022/07-26---triggering-a-github-action-from-an-external-source/#conditionally-waiting-for-the-devblogs-deployment","text":"There is one last thing I have to manage: at the time when this GitHub Action is triggered, the DevBlog's static content was just pushed to the gh-pages branch by MKDocs, so the updated DevBlog is not online yet ! So just doing this is not enough, and we need to wait for the RSS feed to be up-to-date before re-generating the README of the GitHub profile... It seems that most of the time this deployment takes about 30 to 40 seconds, but sometimes it took a little more than a minute; right, let's wait for 90 seconds before reading the RSS feed, and it should do the job. However, I don't want to have this waiting time when I push an updated version of the README file itself, or when I trigger the GitHub Action manually! 15 browser tabs opened on the GH Actions documentation later, I found a solution - name : Wait for devblog deployment on GitHub Pages if : ${{ github.event_name == 'repository_dispatch' && github.event.client_payload.wait_for_deployment }} run : |- echo \"Deployment can take up to 1 minute, let's wait for 90 seconds\" sleep 90 - name : Update README run : |- python build_readme.py cat README.md So the Action will sleep for 90 seconds before running the Python script that fetches the blog's content and update the README accordingly, but only if : The Action was triggered by a \"repository_dispatch\" event. We can detect this with: github.event_name == 'repository_dispatch' The emitter sent a parameter - that I arbitrarily called wait_for_deployment - in its JSON payload, with a truthy value (which is what I did in the curl command earlier ). We can detect this with: github.event.client_payload.wait_for_deployment","title":"Conditionally waiting for the DevBlog's deployment"},{"location":"2022/07-26---triggering-a-github-action-from-an-external-source/#jobs-done","text":"Now when I push to this DevBlog repo it sends a repository_dispatch event to the repository of my GitHub profile, which will trigger the generation of the README after having waited a bit to let some time for the up-to-date RSS feed to be online In the end, all this happens between these 2 YAML files: https://github.com/olivierphi/olivierphi/blob/main/.github/workflows/build.yml https://github.com/olivierphi/devblog/blob/main/.github/workflows/ci.yml","title":"Job's done!"},{"location":"2022/07-28---making-sqlite-much-faster-in-a-local-dev-environment/","tags":["TIL","django","sqlite"],"text":"Making SQLite much faster in a local dev environment \u00b6 Abstract TIL : it is possible to make SQLite data insertions 3 times faster in a \"local development\" environment - where data integrity is not a crucial criteria. UPDATE : It turns out that we can even reach a \" 30 times faster \" gain with additional tweaks - see the update at the end of the article. Inserting data in a SQLite database can be quite slow \u00b6 A Quick \" TIL \" this time... Contrary to my \"Postgres\" habits, for one of my side projects I'm using SQLite quite heavily. Everything works fine, except that... Data insertions are very slow I suspected it could be caused by the extra work SQLite has to do to maintain data integrity, and started to check if it was possible to tune the level of strictness we want for this. SQLite has a \"journal mode\", and it can be customised \u00b6 That's how I found about the journal mode of SQLite: https://www.sqlite.org/pragma.html#pragma_journal_mode We can see on this documentation that it can be set to DELETE , TRUNCATE , PERSIST , MEMORY , WAL or OFF . Right, let's try to change that! At the beginning of the Python function that processes my big batch of data to insert, I've added a SQL query that - given that I understand that doc correctly - should make my data insertions faster: pragma journal_mode = memory ; Let's launch my data insertion script with this update! Result : nothing has changed, the insertion of my \u22482.000 rows still takes 50 seconds Ah, but maybe I need to send this query when the database connection is initiated, rather than on the fly when I need it? Note also that the journal_mode cannot be changed while a transaction is active. The SQLite documentation Setting the \"journal mode\" when the database connection is initialised \u00b6 As I'm using Django for this side project, I had to find a way to send a SQL query as soon as possible to the database, right after the connection is initialised. According to this (old) ticket on the Django bug tracker, which refers to that Stack Overflow thread , a way to do this is to plug a custom function to the signal that Django sends when the connection is initialised, and send the query there. Alright, let's try this! I want to lower the data integrity work only on my \"local dev\" environment, so I'll have to add this code to my project.settings.development module. (explanations about this module in this other post ) # file: src/project/settings/development.py # My existing settings... ... # ...to which I'm adding the following: # Setting SQLite journal mode to 'memory' - much faster writes, # at the expense of database safety and integrity. # @link https://www.sqlite.org/pragma.html#pragma_journal_mode # @link https://code.djangoproject.com/ticket/24018#comment:4 from django.db.backends.signals import connection_created def _disable_sqlite_journal ( sender , connection , ** kwargs ): import logging if connection . vendor == \"sqlite\" : logging . getLogger ( \"apps\" ) . warning ( \"Setting SQLite journal mode to 'memory'\" ) cursor = connection . cursor () cursor . execute ( \"PRAGMA journal_mode = memory;\" ) connection_created . connect ( _disable_sqlite_journal ) Let's launch my data insertion script again! (x2) Result : the insertion of my \u22482.000 rows now takes 15 seconds, instead of 50 ! Closing notes \u00b6 The trade-off is clear: The MEMORY journaling mode stores the rollback journal in volatile RAM. This saves disk I/O but at the expense of database safety and integrity. If the application using SQLite crashes in the middle of a transaction when the MEMORY journaling mode is set, then the database file will very likely go corrupt. The SQLite documentation That's why in my case I want to customise this journal mode only on my local environment , and won't do it in production. But being able to opt in for 3 times faster data insertions is still a pretty good discovery , as being able to iterate quickly is crucial when working on such a local environment! UPDATE \u00b6 Following the publication of this article, the amazing Albin gave me a pointer to this URL: https://avi.im/blag/2021/fast-sqlite-inserts/#sqlite-optimisations If I activate all these settings, the original 50 seconds now become... 1.7 seconds! That's pretty much a \"30 times faster\" gain! from django.db.backends.signals import connection_created _UNLEASH_SQLITE_QUERIES = [ # @link https://avi.im/blag/2021/fast-sqlite-inserts/#sqlite-optimisations \"PRAGMA journal_mode = memory\" , \"PRAGMA synchronous = 0\" , \"PRAGMA cache_size = 1000000\" , \"PRAGMA locking_mode = EXCLUSIVE\" , \"PRAGMA temp_store = MEMORY\" , ] def _disable_sqlite_journal ( sender , connection , ** kwargs ): import logging if connection . vendor == \"sqlite\" : logging . getLogger ( \"apps\" ) . warning ( \"Setting SQLite journal mode to 'memory', and various other settings\" ) cursor = connection . cursor () for sql in _UNLEASH_SQLITE_QUERIES : cursor . execute ( sql ) connection_created . connect ( _disable_sqlite_journal )","title":"Making SQLite much faster in a local dev environment"},{"location":"2022/07-28---making-sqlite-much-faster-in-a-local-dev-environment/#making-sqlite-much-faster-in-a-local-dev-environment","text":"Abstract TIL : it is possible to make SQLite data insertions 3 times faster in a \"local development\" environment - where data integrity is not a crucial criteria. UPDATE : It turns out that we can even reach a \" 30 times faster \" gain with additional tweaks - see the update at the end of the article.","title":"Making SQLite much faster in a local dev environment"},{"location":"2022/07-28---making-sqlite-much-faster-in-a-local-dev-environment/#inserting-data-in-a-sqlite-database-can-be-quite-slow","text":"A Quick \" TIL \" this time... Contrary to my \"Postgres\" habits, for one of my side projects I'm using SQLite quite heavily. Everything works fine, except that... Data insertions are very slow I suspected it could be caused by the extra work SQLite has to do to maintain data integrity, and started to check if it was possible to tune the level of strictness we want for this.","title":"Inserting data in a SQLite database can be quite slow"},{"location":"2022/07-28---making-sqlite-much-faster-in-a-local-dev-environment/#sqlite-has-a-journal-mode-and-it-can-be-customised","text":"That's how I found about the journal mode of SQLite: https://www.sqlite.org/pragma.html#pragma_journal_mode We can see on this documentation that it can be set to DELETE , TRUNCATE , PERSIST , MEMORY , WAL or OFF . Right, let's try to change that! At the beginning of the Python function that processes my big batch of data to insert, I've added a SQL query that - given that I understand that doc correctly - should make my data insertions faster: pragma journal_mode = memory ; Let's launch my data insertion script with this update! Result : nothing has changed, the insertion of my \u22482.000 rows still takes 50 seconds Ah, but maybe I need to send this query when the database connection is initiated, rather than on the fly when I need it? Note also that the journal_mode cannot be changed while a transaction is active. The SQLite documentation","title":"SQLite has a \"journal mode\", and it can be customised"},{"location":"2022/07-28---making-sqlite-much-faster-in-a-local-dev-environment/#setting-the-journal-mode-when-the-database-connection-is-initialised","text":"As I'm using Django for this side project, I had to find a way to send a SQL query as soon as possible to the database, right after the connection is initialised. According to this (old) ticket on the Django bug tracker, which refers to that Stack Overflow thread , a way to do this is to plug a custom function to the signal that Django sends when the connection is initialised, and send the query there. Alright, let's try this! I want to lower the data integrity work only on my \"local dev\" environment, so I'll have to add this code to my project.settings.development module. (explanations about this module in this other post ) # file: src/project/settings/development.py # My existing settings... ... # ...to which I'm adding the following: # Setting SQLite journal mode to 'memory' - much faster writes, # at the expense of database safety and integrity. # @link https://www.sqlite.org/pragma.html#pragma_journal_mode # @link https://code.djangoproject.com/ticket/24018#comment:4 from django.db.backends.signals import connection_created def _disable_sqlite_journal ( sender , connection , ** kwargs ): import logging if connection . vendor == \"sqlite\" : logging . getLogger ( \"apps\" ) . warning ( \"Setting SQLite journal mode to 'memory'\" ) cursor = connection . cursor () cursor . execute ( \"PRAGMA journal_mode = memory;\" ) connection_created . connect ( _disable_sqlite_journal ) Let's launch my data insertion script again! (x2) Result : the insertion of my \u22482.000 rows now takes 15 seconds, instead of 50 !","title":"Setting the \"journal mode\" when the database connection is initialised"},{"location":"2022/07-28---making-sqlite-much-faster-in-a-local-dev-environment/#closing-notes","text":"The trade-off is clear: The MEMORY journaling mode stores the rollback journal in volatile RAM. This saves disk I/O but at the expense of database safety and integrity. If the application using SQLite crashes in the middle of a transaction when the MEMORY journaling mode is set, then the database file will very likely go corrupt. The SQLite documentation That's why in my case I want to customise this journal mode only on my local environment , and won't do it in production. But being able to opt in for 3 times faster data insertions is still a pretty good discovery , as being able to iterate quickly is crucial when working on such a local environment!","title":"Closing notes"},{"location":"2022/07-28---making-sqlite-much-faster-in-a-local-dev-environment/#update","text":"Following the publication of this article, the amazing Albin gave me a pointer to this URL: https://avi.im/blag/2021/fast-sqlite-inserts/#sqlite-optimisations If I activate all these settings, the original 50 seconds now become... 1.7 seconds! That's pretty much a \"30 times faster\" gain! from django.db.backends.signals import connection_created _UNLEASH_SQLITE_QUERIES = [ # @link https://avi.im/blag/2021/fast-sqlite-inserts/#sqlite-optimisations \"PRAGMA journal_mode = memory\" , \"PRAGMA synchronous = 0\" , \"PRAGMA cache_size = 1000000\" , \"PRAGMA locking_mode = EXCLUSIVE\" , \"PRAGMA temp_store = MEMORY\" , ] def _disable_sqlite_journal ( sender , connection , ** kwargs ): import logging if connection . vendor == \"sqlite\" : logging . getLogger ( \"apps\" ) . warning ( \"Setting SQLite journal mode to 'memory', and various other settings\" ) cursor = connection . cursor () for sql in _UNLEASH_SQLITE_QUERIES : cursor . execute ( sql ) connection_created . connect ( _disable_sqlite_journal )","title":"UPDATE"},{"location":"tags/","text":"Tags \u00b6 Following is a list of relevant tags: TIL \u00b6 Triggering a GitHub Action from an external source Making SQLite much faster in a local dev environment blog \u00b6 Building a Markdown-based blog django \u00b6 Choosing a tech stack for my card game platform 'From scratch to online in production' in a single day, with Django - Part 1 'From scratch to online in production' in a single day, with Django - Part 2 Making SQLite much faster in a local dev environment github \u00b6 Triggering a GitHub Action from an external source golang \u00b6 Choosing a tech stack for my card game platform laravel \u00b6 Choosing a tech stack for my card game platform mkdocs \u00b6 Building a Markdown-based blog next.js \u00b6 Choosing a tech stack for my card game platform project layout \u00b6 'From scratch to online in production' in a single day, with Django - Part 1 'From scratch to online in production' in a single day, with Django - Part 2 python \u00b6 Building a Markdown-based blog rails \u00b6 Choosing a tech stack for my card game platform sqlite \u00b6 Making SQLite much faster in a local dev environment","title":"Tags"},{"location":"tags/#tags","text":"Following is a list of relevant tags:","title":"Tags"},{"location":"tags/#til","text":"Triggering a GitHub Action from an external source Making SQLite much faster in a local dev environment","title":"TIL"},{"location":"tags/#blog","text":"Building a Markdown-based blog","title":"blog"},{"location":"tags/#django","text":"Choosing a tech stack for my card game platform 'From scratch to online in production' in a single day, with Django - Part 1 'From scratch to online in production' in a single day, with Django - Part 2 Making SQLite much faster in a local dev environment","title":"django"},{"location":"tags/#github","text":"Triggering a GitHub Action from an external source","title":"github"},{"location":"tags/#golang","text":"Choosing a tech stack for my card game platform","title":"golang"},{"location":"tags/#laravel","text":"Choosing a tech stack for my card game platform","title":"laravel"},{"location":"tags/#mkdocs","text":"Building a Markdown-based blog","title":"mkdocs"},{"location":"tags/#nextjs","text":"Choosing a tech stack for my card game platform","title":"next.js"},{"location":"tags/#project-layout","text":"'From scratch to online in production' in a single day, with Django - Part 1 'From scratch to online in production' in a single day, with Django - Part 2","title":"project layout"},{"location":"tags/#python","text":"Building a Markdown-based blog","title":"python"},{"location":"tags/#rails","text":"Choosing a tech stack for my card game platform","title":"rails"},{"location":"tags/#sqlite","text":"Making SQLite much faster in a local dev environment","title":"sqlite"}]}